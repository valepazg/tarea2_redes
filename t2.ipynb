{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rLfIEI2ZI5c"
   },
   "source": [
    "## Instalar Librerías e Importar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wD980A30LRmC",
    "outputId": "b9a995fa-db56-481f-d3cc-b4c88a2bd126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'aif360[Reductions]'\"\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo tensorflow aif360 'aif360[Reductions]' 'aif360[inFairness]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IMyZSOOsZa1X",
    "outputId": "a22ff9b8-085d-4e9a-b25a-ee3f74b38b9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4cOKrsx6Lk3m",
    "outputId": "a0153edf-245a-4111-dd16-d72b3f595d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "# metadata\n",
    "print(adult.metadata)\n",
    "\n",
    "# variable information\n",
    "print(adult.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucfynq5YaWd-"
   },
   "source": [
    "## Preparar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "_3ppeC1oMN2Z",
    "outputId": "d5dbb815-f7da-40a5-edeb-72358ddb8100"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_labels(label: str):\n",
    "    return label.replace(\".\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "y_df = pd.DataFrame(y)\n",
    "\n",
    "y_df = y_df.map(clean_labels)\n",
    "\n",
    "df = pd.concat([X_df, y_df], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47621, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ssZ8SjneQN8X"
   },
   "outputs": [],
   "source": [
    "X_df_clean = df.drop(columns=['income'])\n",
    "y_df_clean = df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QS-zM6tTMqP2"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df_clean, y_df_clean, test_size=0.3, stratify=y_df_clean, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=X_df_clean.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_df_clean.columns)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns=['income'])\n",
    "y_test = pd.DataFrame(y_test, columns=['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex map\n",
    "mapping_sex = {\"Female\": 0, \"Male\": 1}\n",
    "# Income map\n",
    "mapping_income = {\"<=50K\": 0, \">50K\": 1}\n",
    "\n",
    "def map_col(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    mapping: dict,\n",
    ") -> pd.DataFrame:\n",
    "    df[col] = df[col].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing train data\n",
    "X_train['age'] = X_train['age'].apply(lambda x: 1 if x < 60 else 0)\n",
    "X_train = map_col(X_train, \"sex\", mapping_sex)\n",
    "\n",
    "y_train = map_col(y_train, \"income\", mapping_income)\n",
    "\n",
    "# Preprocessing test data\n",
    "X_test['age'] = X_test['age'].apply(lambda x: 1 if x < 60 else 0)\n",
    "X_test = map_col(X_test, \"sex\", mapping_sex)\n",
    "\n",
    "y_test = map_col(y_test, \"income\", mapping_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26818</th>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>202033</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28252</th>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>340599</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9258</th>\n",
       "      <td>1</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>177035</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41064</th>\n",
       "      <td>1</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>30012</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30461</th>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>185216</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt     education  education-num  \\\n",
       "26818    1           Private  202033     Bachelors             13   \n",
       "28252    1           Private  340599          11th              7   \n",
       "9258     1         State-gov  177035          11th              7   \n",
       "41064    1  Self-emp-not-inc   30012  Some-college             10   \n",
       "30461    1           Private  185216  Some-college             10   \n",
       "\n",
       "           marital-status      occupation relationship   race  sex  \\\n",
       "26818  Married-civ-spouse  Prof-specialty      Husband  White    1   \n",
       "28252           Separated   Other-service    Unmarried  Black    0   \n",
       "9258             Divorced   Other-service    Unmarried  White    0   \n",
       "41064  Married-civ-spouse           Sales      Husband  White    1   \n",
       "30461       Never-married    Adm-clerical    Own-child  White    1   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country  \n",
       "26818             0             0              40  United-States  \n",
       "28252             0             0              40  United-States  \n",
       "9258              0             0              40  United-States  \n",
       "41064             0             0              50  United-States  \n",
       "30461             0             0              40  United-States  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26818</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28252</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9258</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41064</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30461</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       income\n",
       "26818       0\n",
       "28252       0\n",
       "9258        0\n",
       "41064       1\n",
       "30461       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33334, 107), (14287, 107))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "standard_scaler = MinMaxScaler()\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Aplicamos directamente a los datos\n",
    "X_numeric_train_scaled = standard_scaler.fit_transform(X_train[numeric_cols])\n",
    "X_numeric_test_scaled = standard_scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Aplicamos one hot encoding\n",
    "X_categorical_train = one_hot_encoder.fit_transform(X_train[categorical_cols])\n",
    "X_categorical_test = one_hot_encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "X_train_processed = np.concatenate([X_numeric_train_scaled, X_categorical_train.toarray()], axis=1)\n",
    "X_test_processed = np.concatenate([X_numeric_test_scaled, X_categorical_test.toarray()], axis=1)\n",
    "X_train_processed.shape, X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() + one_hot_encoder.get_feature_names_out(categorical_cols).tolist())\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=numeric_cols.tolist() + one_hot_encoder.get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_?</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.127659</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221481</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110733</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116273</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age    fnlwgt  education-num  sex  capital-gain  capital-loss  \\\n",
       "0  1.0  0.127659            0.8  1.0           0.0           0.0   \n",
       "1  1.0  0.221481            0.4  0.0           0.0           0.0   \n",
       "2  1.0  0.110733            0.4  0.0           0.0           0.0   \n",
       "3  1.0  0.011186            0.6  1.0           0.0           0.0   \n",
       "4  1.0  0.116273            0.6  1.0           0.0           0.0   \n",
       "\n",
       "   hours-per-week  workclass_?  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0        0.397959          0.0                    0.0                  0.0   \n",
       "1        0.397959          0.0                    0.0                  0.0   \n",
       "2        0.397959          0.0                    0.0                  0.0   \n",
       "3        0.500000          0.0                    0.0                  0.0   \n",
       "4        0.397959          0.0                    0.0                  0.0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                      0.0                         0.0   \n",
       "1  ...                      0.0                         0.0   \n",
       "2  ...                      0.0                         0.0   \n",
       "3  ...                      0.0                         0.0   \n",
       "4  ...                      0.0                         0.0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                      0.0                   0.0                    0.0   \n",
       "1                      0.0                   0.0                    0.0   \n",
       "2                      0.0                   0.0                    0.0   \n",
       "3                      0.0                   0.0                    0.0   \n",
       "4                      0.0                   0.0                    0.0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                      0.0                             0.0   \n",
       "1                      0.0                             0.0   \n",
       "2                      0.0                             0.0   \n",
       "3                      0.0                             0.0   \n",
       "4                      0.0                             0.0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                           1.0                     0.0   \n",
       "1                           1.0                     0.0   \n",
       "2                           1.0                     0.0   \n",
       "3                           1.0                     0.0   \n",
       "4                           1.0                     0.0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                        0.0  \n",
       "1                        0.0  \n",
       "2                        0.0  \n",
       "3                        0.0  \n",
       "4                        0.0  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFD_er0Qay6x"
   },
   "source": [
    "# Entrenamiento de Modelos\n",
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfCR6vvnR_0f",
    "outputId": "c3294992-cc9f-482f-fb69-68d75c94805a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression Model: 0.85\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "logistic_regression.fit(X_train_df, y_train)\n",
    "logistic_pred = logistic_regression.predict(X_test_df)\n",
    "acc_lr = accuracy_score(y_test, logistic_pred)\n",
    "print(f'Accuracy for Logistic Regression Model: {acc_lr:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvPP0KmQbYo_"
   },
   "source": [
    "### Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcmXK0xCSO71",
    "outputId": "41e94751-b10f-4658-ff9e-9468d12062fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest Classifier Model: 0.83\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(X_train_df, y_train)\n",
    "randomF_pred = random_forest.predict(X_test_df)\n",
    "acc_rf = accuracy_score(y_test, randomF_pred)\n",
    "print(f'Accuracy for Random Forest Classifier Model: {acc_rf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KcElJCtb0sn"
   },
   "source": [
    "### K Neighbors Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQ-ljNymSvSC",
    "outputId": "c77636bf-61ea-4841-c284-4b4a8cc7c75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for K Neighbors Classifier Model: 0.82\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train_df, y_train)\n",
    "knn_pred = knn.predict(X_test_df)\n",
    "acc_kn = accuracy_score(y_test, knn_pred)\n",
    "print(f'Accuracy for K Neighbors Classifier Model: {acc_kn:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp5ENmaTcb00"
   },
   "source": [
    "# Métricas de Equidad\n",
    "#### Modelo escogido por mejor rendimiento: **Logistic Regression Model**\n",
    "#### Atributos sensibles a estudiar: **age** y **sex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukUHvkSTdIlI"
   },
   "source": [
    "## Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dud7uJs4EdZX"
   },
   "outputs": [],
   "source": [
    "best_pred = logistic_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "y1gE7xoaG2Cf"
   },
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "df_test_aif = BinaryLabelDataset(\n",
    "    df=pd.concat([X_test_df, pd.DataFrame(y_test, columns=['income'])], axis=1),\n",
    "    label_names=['income'],\n",
    "    protected_attribute_names=['age', 'sex'],\n",
    ")\n",
    "\n",
    "df_test_pred_aif = df_test_aif.copy(deepcopy=True)\n",
    "df_test_pred_aif.labels = best_pred.reshape(-1, 1)\n",
    "\n",
    "df_train_aif = BinaryLabelDataset(\n",
    "    df=pd.concat([X_train_df, pd.DataFrame(y_train, columns=['income'])], axis=1),\n",
    "    label_names=['income'],\n",
    "    protected_attribute_names=['age', 'sex'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e5yJhS1-mdM",
    "outputId": "fa00d4a0-9beb-4e1d-d23a-8fb73bd3c5e3"
   },
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "\n",
    "def get_demographic_parity_metrics(\n",
    "    test_aif: BinaryLabelDataset,\n",
    "    test_pred_aif: BinaryLabelDataset,\n",
    "):\n",
    "    # Para 'age'\n",
    "    metric_age = ClassificationMetric(\n",
    "        test_aif,\n",
    "        test_pred_aif,\n",
    "        privileged_groups=[{\"age\": 1}],\n",
    "        unprivileged_groups=[{\"age\": 0}]\n",
    "    )\n",
    "\n",
    "    # Para 'sex'\n",
    "    metric_sex = ClassificationMetric(\n",
    "        test_aif,\n",
    "        test_pred_aif,\n",
    "        privileged_groups=[{\"sex\": 1}],\n",
    "        unprivileged_groups=[{\"sex\": 0}]\n",
    "    )\n",
    "\n",
    "    print(\"Disparate Impact for age:\", metric_age.disparate_impact())\n",
    "    print(\"Disparate Impact for sex:\", metric_sex.disparate_impact())\n",
    "    \n",
    "    return metric_age, metric_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.050856421312152\n",
      "Disparate Impact for sex: 0.2816774955165621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<aif360.metrics.classification_metric.ClassificationMetric at 0x1653b632030>,\n",
       " <aif360.metrics.classification_metric.ClassificationMetric at 0x1653b5e6db0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_demographic_parity_metrics(df_test_aif, df_test_pred_aif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcMp7nFSdUQp"
   },
   "source": [
    "## Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3Yr05DpXe-7n"
   },
   "outputs": [],
   "source": [
    "def map_col(df, col, mapping) -> pd.DataFrame:\n",
    "    df[col] = df[col].map(mapping)\n",
    "    return df\n",
    "\n",
    "def calculate_tpr_fpr_sex(data, subgroup):\n",
    "    group_data = data[data['sex'] == subgroup]\n",
    "    true_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 1))\n",
    "    false_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 0))\n",
    "    total_positive = np.sum(group_data['y_true'] == 1)\n",
    "    total_negative = np.sum(group_data['y_true'] == 0)\n",
    "\n",
    "    tpr = true_positive / total_positive if total_positive > 0 else 0\n",
    "    fpr = false_positive / total_negative if total_negative > 0 else 0\n",
    "\n",
    "    return tpr, fpr\n",
    "\n",
    "def calculate_tpr_fpr_age(data, subgroup):\n",
    "    group_data = data[data['age'] == subgroup]\n",
    "    true_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 1))\n",
    "    false_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 0))\n",
    "    total_positive = np.sum(group_data['y_true'] == 1)\n",
    "    total_negative = np.sum(group_data['y_true'] == 0)\n",
    "\n",
    "    tpr = true_positive / total_positive if total_positive > 0 else 0\n",
    "    fpr = false_positive / total_negative if total_negative > 0 else 0\n",
    "\n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_to_separacion(X_test_dataframe, y_test, y_pred):\n",
    "    return pd.concat([\n",
    "        X_test_dataframe,\n",
    "        pd.DataFrame(y_test, columns=['y_true']),\n",
    "        pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGLRTJKJgNrj"
   },
   "source": [
    "### Equalized Odds for **age** and **sex** atribute\n",
    "\n",
    "Map: [**0: Older Adults**, **1: Young Adults**]\n",
    "\n",
    "Map: [**0: Female**, **1: Male**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkQqJSfzIPTO",
    "outputId": "fef17477-567c-48b6-a567-46085afa7dd5"
   },
   "outputs": [],
   "source": [
    "def get_equalized_odds_metrics(\n",
    "    X_df: pd.DataFrame,\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "):\n",
    "    separacion_df = prepare_df_to_separacion(X_df, y_true, y_pred)\n",
    "    for age in [0, 1]:\n",
    "        tpr, fpr = calculate_tpr_fpr_age(separacion_df, age)\n",
    "        age_label = 'Older Adults' if age == 0 else 'Young Adults'\n",
    "        print(f\"Age Group: {age_label}\")\n",
    "        print(f\"  True Positive Rate (TPR): {tpr:.2f}\")\n",
    "        print(f\"  False Positive Rate (FPR): {fpr:.2f}\")\n",
    "        \n",
    "    for sex in [0, 1]:\n",
    "        tpr, fpr = calculate_tpr_fpr_sex(separacion_df, sex)\n",
    "        sex_label = 'Female' if sex == 0 else 'Male'\n",
    "        print(f\"Sex Group: {sex_label}\")\n",
    "        print(f\"  True Positive Rate (TPR): {tpr:.2f}\")\n",
    "        print(f\"  False Positive Rate (FPR): {fpr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Older Adults\n",
      "  True Positive Rate (TPR): 0.57\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Age Group: Young Adults\n",
      "  True Positive Rate (TPR): 0.59\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Sex Group: Female\n",
      "  True Positive Rate (TPR): 0.45\n",
      "  False Positive Rate (FPR): 0.02\n",
      "Sex Group: Male\n",
      "  True Positive Rate (TPR): 0.61\n",
      "  False Positive Rate (FPR): 0.10\n"
     ]
    }
   ],
   "source": [
    "get_equalized_odds_metrics(X_test_df, y_test, best_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtnPwLPQnAfm"
   },
   "source": [
    "## Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQAWZgLBKl1-",
    "outputId": "a3bccf47-cae9-4d93-b419-0a617501118d"
   },
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "\n",
    "def get_predictive_parity_metrics(\n",
    "    test_aif: BinaryLabelDataset,\n",
    "    test_pred_aif: BinaryLabelDataset,\n",
    "):\n",
    "    # Para 'age'\n",
    "    metric_age = ClassificationMetric(\n",
    "        test_aif,\n",
    "        test_pred_aif,\n",
    "        privileged_groups=[{\"age\": 1}],\n",
    "        unprivileged_groups=[{\"age\": 0}]\n",
    "    )\n",
    "    \n",
    "    # Para 'sex'\n",
    "    metric_sex = ClassificationMetric(\n",
    "        df_test_aif,\n",
    "        df_test_pred_aif,\n",
    "        privileged_groups=[{\"sex\": 1}],\n",
    "        unprivileged_groups=[{\"sex\": 0}]\n",
    "    )\n",
    "\n",
    "    # Calcular el Valor Predictivo Positivo (PPV) para 'age'\n",
    "    privileged_ppv_age = metric_age.positive_predictive_value(privileged=True)\n",
    "    unprivileged_ppv_age = metric_age.positive_predictive_value(privileged=False)\n",
    "\n",
    "    # Calcular el Valor Predictivo Positivo (PPV) para 'sex'\n",
    "    privileged_ppv_sex = metric_sex.positive_predictive_value(privileged=True)\n",
    "    unprivileged_ppv_sex = metric_sex.positive_predictive_value(privileged=False)\n",
    "\n",
    "    # Calcular la diferencia de paridad predictiva\n",
    "    predictive_parity_difference_sex = privileged_ppv_sex - unprivileged_ppv_sex\n",
    "    predictive_parity_difference_age = privileged_ppv_age - unprivileged_ppv_age\n",
    "    \n",
    "    print(f\"Privileged PPV age: {privileged_ppv_age}\")\n",
    "    print(f\"Unprivileged PPV age: {unprivileged_ppv_age}\")\n",
    "    print(f\"Predictive Parity Difference age: {predictive_parity_difference_age}\")\n",
    "\n",
    "    print(f\"Privileged PPV sex: {privileged_ppv_sex}\")\n",
    "    print(f\"Unprivileged PPV sex: {unprivileged_ppv_sex}\")\n",
    "    print(f\"Predictive Parity Difference sex: {predictive_parity_difference_sex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged PPV age: 0.7272727272727273\n",
      "Unprivileged PPV age: 0.7467248908296943\n",
      "Predictive Parity Difference age: -0.019452163556966995\n",
      "Privileged PPV sex: 0.7310683585755219\n",
      "Unprivileged PPV sex: 0.7130177514792899\n",
      "Predictive Parity Difference sex: 0.018050607096231963\n"
     ]
    }
   ],
   "source": [
    "get_predictive_parity_metrics(df_test_aif, df_test_pred_aif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WENKgyhh44_"
   },
   "source": [
    "# Mitigación de Sesgos\n",
    "### Pre-procesamiento: **Reweighing**\n",
    "### In-procesamiento:\n",
    "### Post-procesamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFcG9Ttaicoi"
   },
   "source": [
    "## Pre-procesamiento: **Reweighing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevos pesos tras Reweighing:\n",
      "[1.09493593 0.8507266  0.8507266  0.78650541 1.09493593 1.09493593\n",
      " 1.09493593 0.8507266  1.09493593 0.8507266 ]\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from typing import List\n",
    "\n",
    "def reweighingPreprocessing(\n",
    "    train_aif_df: BinaryLabelDataset,\n",
    "    sensitive_features: List[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Aplica Reweighing a un conjunto de entrenamiento considerando múltiples atributos sensibles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear procesadores Reweighing para cada atributo sensible\n",
    "    reweigh_processors = []\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        reweigh_processors.append(\n",
    "            Reweighing(\n",
    "                unprivileged_groups=[{sensitive_feature: 0}],\n",
    "                privileged_groups=[{sensitive_feature: 1}]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Aplicar Reweighing a los pesos del dataset\n",
    "    actual_train_df = train_aif_df.copy()\n",
    "    \n",
    "    for reweigh_processor in reweigh_processors:\n",
    "        reweigh_processor: Reweighing\n",
    "        reweigh_processor.fit(actual_train_df)\n",
    "        actual_train_df = reweigh_processor.transform(actual_train_df)\n",
    "    \n",
    "    return actual_train_df\n",
    "\n",
    "# Aplicar Reweighing\n",
    "reweighted_train = reweighingPreprocessing(\n",
    "    df_train_aif,\n",
    "    ['age', 'sex']\n",
    ")\n",
    "\n",
    "# Mostrar los nuevos pesos después del preprocesamiento\n",
    "print(\"Nuevos pesos tras Reweighing:\")\n",
    "print(reweighted_train.instance_weights[:10])  # Muestra los primeros 10 ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression Model with Pre-processing: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logistic_model_pre = LogisticRegression(max_iter=5000, random_state=42)\n",
    "logistic_model_pre.fit(\n",
    "    reweighted_train.features, \n",
    "    reweighted_train.labels.ravel(), \n",
    "    sample_weight=reweighted_train.instance_weights\n",
    ")\n",
    "y_pred_logistic_pre = logistic_model_pre.predict(X_test_df)\n",
    "acc_lr_pre = accuracy_score(y_test, y_pred_logistic_pre)\n",
    "print(f'Accuracy for Logistic Regression Model with Pre-processing: {acc_lr_pre:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rS3vu7C2CEZ"
   },
   "source": [
    "## In-procesamiento: **Inserte Técnica Aquí**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import tensorflow as tf \n",
    "\n",
    "def adversarialDebiasingProcessing(\n",
    "    train_aif_df: BinaryLabelDataset,\n",
    "    test_aif_df: BinaryLabelDataset,\n",
    "    sensitive_features: List[str],\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 128,\n",
    "    adversary_loss_weight: float = 0.1,\n",
    "    name_run_identifier: str = 'adversarial_debiasing',\n",
    "):\n",
    "    \"\"\"\n",
    "    Función para entrenar un modelo debiasado usando AdversarialDebiasing.\n",
    "\n",
    "    Parámetros:\n",
    "    - train_aif_df (BinaryLabelDataset): Datos de entrenamiento.\n",
    "    - test_aif_df (BinaryLabelDataset): Datos de prueba.\n",
    "    - sensitive_features (List[str]): Lista de características sensibles (e.g., ['age', 'sex']).\n",
    "    - num_epochs (int): Número de épocas para entrenar.\n",
    "    - batch_size (int): Tamaño de lote durante el entrenamiento.\n",
    "    - adversary_loss_weight (float): Peso del adversario para reducir el sesgo.\n",
    "\n",
    "    Retorno:\n",
    "    - predicted_test_aif_df (BinaryLabelDataset): Predicciones debiasadas en el conjunto de prueba.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    sess = tf.compat.v1.Session()\n",
    "    debiased_models = []\n",
    "    predicted_test_aif_df = test_aif_df.copy()\n",
    "\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        # Configurar grupos privilegiados y no privilegiados\n",
    "        privileged_groups = [{sensitive_feature: 1}]\n",
    "        unprivileged_groups = [{sensitive_feature: 0}]\n",
    "        \n",
    "        # Crear y entrenar el modelo debiasado\n",
    "        debiased_model = AdversarialDebiasing(\n",
    "            privileged_groups=privileged_groups,\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            scope_name=f'{name_run_identifier}_{sensitive_feature}',\n",
    "            sess=sess,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            adversary_loss_weight=adversary_loss_weight\n",
    "        )\n",
    "        print(f\"Entrenando modelo debiasado para {sensitive_feature}...\")\n",
    "        debiased_model.fit(train_aif_df)\n",
    "        debiased_models.append(debiased_model)\n",
    "\n",
    "        # Generar predicciones\n",
    "        predicted_test_aif_df = debiased_model.predict(predicted_test_aif_df)\n",
    "\n",
    "    sess.close()\n",
    "    return predicted_test_aif_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_15200\\4030671838.py:31: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_15200\\4030671838.py:31: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo debiasado para age...\n",
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.693852; batch adversarial loss: 0.657653\n",
      "epoch 0; iter: 200; batch classifier loss: 0.461706; batch adversarial loss: 0.582487\n",
      "epoch 1; iter: 0; batch classifier loss: 0.439766; batch adversarial loss: 0.563201\n",
      "epoch 1; iter: 200; batch classifier loss: 0.464464; batch adversarial loss: 0.539654\n",
      "epoch 2; iter: 0; batch classifier loss: 0.353233; batch adversarial loss: 0.550909\n",
      "epoch 2; iter: 200; batch classifier loss: 0.443546; batch adversarial loss: 0.473871\n",
      "epoch 3; iter: 0; batch classifier loss: 0.454370; batch adversarial loss: 0.421975\n",
      "epoch 3; iter: 200; batch classifier loss: 0.513054; batch adversarial loss: 0.461757\n",
      "epoch 4; iter: 0; batch classifier loss: 0.553348; batch adversarial loss: 0.377774\n",
      "epoch 4; iter: 200; batch classifier loss: 0.386324; batch adversarial loss: 0.418895\n",
      "epoch 5; iter: 0; batch classifier loss: 0.428137; batch adversarial loss: 0.354470\n",
      "epoch 5; iter: 200; batch classifier loss: 0.260522; batch adversarial loss: 0.389833\n",
      "epoch 6; iter: 0; batch classifier loss: 0.414810; batch adversarial loss: 0.395676\n",
      "epoch 6; iter: 200; batch classifier loss: 0.328480; batch adversarial loss: 0.290050\n",
      "epoch 7; iter: 0; batch classifier loss: 0.396447; batch adversarial loss: 0.293532\n",
      "epoch 7; iter: 200; batch classifier loss: 0.320449; batch adversarial loss: 0.332886\n",
      "epoch 8; iter: 0; batch classifier loss: 0.345470; batch adversarial loss: 0.282602\n",
      "epoch 8; iter: 200; batch classifier loss: 0.290334; batch adversarial loss: 0.328563\n",
      "epoch 9; iter: 0; batch classifier loss: 0.226528; batch adversarial loss: 0.323732\n",
      "epoch 9; iter: 200; batch classifier loss: 0.333781; batch adversarial loss: 0.315241\n",
      "epoch 10; iter: 0; batch classifier loss: 0.370937; batch adversarial loss: 0.371310\n",
      "epoch 10; iter: 200; batch classifier loss: 0.292337; batch adversarial loss: 0.329107\n",
      "epoch 11; iter: 0; batch classifier loss: 0.272365; batch adversarial loss: 0.299837\n",
      "epoch 11; iter: 200; batch classifier loss: 0.270127; batch adversarial loss: 0.209135\n",
      "epoch 12; iter: 0; batch classifier loss: 0.315566; batch adversarial loss: 0.365236\n",
      "epoch 12; iter: 200; batch classifier loss: 0.272858; batch adversarial loss: 0.280086\n",
      "epoch 13; iter: 0; batch classifier loss: 0.278124; batch adversarial loss: 0.345195\n",
      "epoch 13; iter: 200; batch classifier loss: 0.400965; batch adversarial loss: 0.230349\n",
      "epoch 14; iter: 0; batch classifier loss: 0.243493; batch adversarial loss: 0.263604\n",
      "epoch 14; iter: 200; batch classifier loss: 0.337105; batch adversarial loss: 0.280434\n",
      "epoch 15; iter: 0; batch classifier loss: 0.390506; batch adversarial loss: 0.277644\n",
      "epoch 15; iter: 200; batch classifier loss: 0.288189; batch adversarial loss: 0.297019\n",
      "epoch 16; iter: 0; batch classifier loss: 0.336304; batch adversarial loss: 0.294998\n",
      "epoch 16; iter: 200; batch classifier loss: 0.389587; batch adversarial loss: 0.331904\n",
      "epoch 17; iter: 0; batch classifier loss: 0.317905; batch adversarial loss: 0.273628\n",
      "epoch 17; iter: 200; batch classifier loss: 0.315814; batch adversarial loss: 0.243057\n",
      "epoch 18; iter: 0; batch classifier loss: 0.318386; batch adversarial loss: 0.288868\n",
      "epoch 18; iter: 200; batch classifier loss: 0.393589; batch adversarial loss: 0.307838\n",
      "epoch 19; iter: 0; batch classifier loss: 0.353206; batch adversarial loss: 0.315486\n",
      "epoch 19; iter: 200; batch classifier loss: 0.407783; batch adversarial loss: 0.291103\n",
      "epoch 20; iter: 0; batch classifier loss: 0.348025; batch adversarial loss: 0.219567\n",
      "epoch 20; iter: 200; batch classifier loss: 0.227125; batch adversarial loss: 0.252797\n",
      "epoch 21; iter: 0; batch classifier loss: 0.247377; batch adversarial loss: 0.237339\n",
      "epoch 21; iter: 200; batch classifier loss: 0.327095; batch adversarial loss: 0.258287\n",
      "epoch 22; iter: 0; batch classifier loss: 0.314927; batch adversarial loss: 0.180872\n",
      "epoch 22; iter: 200; batch classifier loss: 0.301446; batch adversarial loss: 0.349861\n",
      "epoch 23; iter: 0; batch classifier loss: 0.253744; batch adversarial loss: 0.216889\n",
      "epoch 23; iter: 200; batch classifier loss: 0.363135; batch adversarial loss: 0.272511\n",
      "epoch 24; iter: 0; batch classifier loss: 0.388660; batch adversarial loss: 0.292000\n",
      "epoch 24; iter: 200; batch classifier loss: 0.246908; batch adversarial loss: 0.274205\n",
      "epoch 25; iter: 0; batch classifier loss: 0.301117; batch adversarial loss: 0.199091\n",
      "epoch 25; iter: 200; batch classifier loss: 0.227002; batch adversarial loss: 0.273059\n",
      "epoch 26; iter: 0; batch classifier loss: 0.226591; batch adversarial loss: 0.178899\n",
      "epoch 26; iter: 200; batch classifier loss: 0.317713; batch adversarial loss: 0.314166\n",
      "epoch 27; iter: 0; batch classifier loss: 0.328377; batch adversarial loss: 0.333598\n",
      "epoch 27; iter: 200; batch classifier loss: 0.251746; batch adversarial loss: 0.254548\n",
      "epoch 28; iter: 0; batch classifier loss: 0.286691; batch adversarial loss: 0.273254\n",
      "epoch 28; iter: 200; batch classifier loss: 0.325901; batch adversarial loss: 0.235441\n",
      "epoch 29; iter: 0; batch classifier loss: 0.292363; batch adversarial loss: 0.291695\n",
      "epoch 29; iter: 200; batch classifier loss: 0.361189; batch adversarial loss: 0.236839\n",
      "epoch 30; iter: 0; batch classifier loss: 0.316838; batch adversarial loss: 0.292924\n",
      "epoch 30; iter: 200; batch classifier loss: 0.216171; batch adversarial loss: 0.312368\n",
      "epoch 31; iter: 0; batch classifier loss: 0.344413; batch adversarial loss: 0.331651\n",
      "epoch 31; iter: 200; batch classifier loss: 0.297370; batch adversarial loss: 0.312453\n",
      "epoch 32; iter: 0; batch classifier loss: 0.262619; batch adversarial loss: 0.256734\n",
      "epoch 32; iter: 200; batch classifier loss: 0.214341; batch adversarial loss: 0.254861\n",
      "epoch 33; iter: 0; batch classifier loss: 0.307444; batch adversarial loss: 0.197978\n",
      "epoch 33; iter: 200; batch classifier loss: 0.252997; batch adversarial loss: 0.274334\n",
      "epoch 34; iter: 0; batch classifier loss: 0.262205; batch adversarial loss: 0.254992\n",
      "epoch 34; iter: 200; batch classifier loss: 0.344713; batch adversarial loss: 0.275364\n",
      "epoch 35; iter: 0; batch classifier loss: 0.287485; batch adversarial loss: 0.387984\n",
      "epoch 35; iter: 200; batch classifier loss: 0.271080; batch adversarial loss: 0.198008\n",
      "epoch 36; iter: 0; batch classifier loss: 0.246793; batch adversarial loss: 0.330576\n",
      "epoch 36; iter: 200; batch classifier loss: 0.303603; batch adversarial loss: 0.217185\n",
      "epoch 37; iter: 0; batch classifier loss: 0.240078; batch adversarial loss: 0.332169\n",
      "epoch 37; iter: 200; batch classifier loss: 0.230826; batch adversarial loss: 0.254664\n",
      "epoch 38; iter: 0; batch classifier loss: 0.257085; batch adversarial loss: 0.293326\n",
      "epoch 38; iter: 200; batch classifier loss: 0.244684; batch adversarial loss: 0.254558\n",
      "epoch 39; iter: 0; batch classifier loss: 0.201350; batch adversarial loss: 0.217475\n",
      "epoch 39; iter: 200; batch classifier loss: 0.219362; batch adversarial loss: 0.198962\n",
      "epoch 40; iter: 0; batch classifier loss: 0.240279; batch adversarial loss: 0.236386\n",
      "epoch 40; iter: 200; batch classifier loss: 0.250382; batch adversarial loss: 0.312519\n",
      "epoch 41; iter: 0; batch classifier loss: 0.270997; batch adversarial loss: 0.444770\n",
      "epoch 41; iter: 200; batch classifier loss: 0.239909; batch adversarial loss: 0.236175\n",
      "epoch 42; iter: 0; batch classifier loss: 0.312930; batch adversarial loss: 0.293148\n",
      "epoch 42; iter: 200; batch classifier loss: 0.259752; batch adversarial loss: 0.254993\n",
      "epoch 43; iter: 0; batch classifier loss: 0.293833; batch adversarial loss: 0.274415\n",
      "epoch 43; iter: 200; batch classifier loss: 0.310270; batch adversarial loss: 0.236558\n",
      "epoch 44; iter: 0; batch classifier loss: 0.324083; batch adversarial loss: 0.255031\n",
      "epoch 44; iter: 200; batch classifier loss: 0.292477; batch adversarial loss: 0.312799\n",
      "epoch 45; iter: 0; batch classifier loss: 0.219723; batch adversarial loss: 0.292854\n",
      "epoch 45; iter: 200; batch classifier loss: 0.340135; batch adversarial loss: 0.236353\n",
      "epoch 46; iter: 0; batch classifier loss: 0.197978; batch adversarial loss: 0.255281\n",
      "epoch 46; iter: 200; batch classifier loss: 0.281041; batch adversarial loss: 0.331930\n",
      "epoch 47; iter: 0; batch classifier loss: 0.306568; batch adversarial loss: 0.254881\n",
      "epoch 47; iter: 200; batch classifier loss: 0.318687; batch adversarial loss: 0.293150\n",
      "epoch 48; iter: 0; batch classifier loss: 0.340780; batch adversarial loss: 0.198160\n",
      "epoch 48; iter: 200; batch classifier loss: 0.223655; batch adversarial loss: 0.256352\n",
      "epoch 49; iter: 0; batch classifier loss: 0.255971; batch adversarial loss: 0.255283\n",
      "epoch 49; iter: 200; batch classifier loss: 0.306520; batch adversarial loss: 0.314653\n",
      "Entrenando modelo debiasado para sex...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.620385; batch adversarial loss: 0.778069\n",
      "epoch 0; iter: 200; batch classifier loss: 0.411770; batch adversarial loss: 0.770170\n",
      "epoch 1; iter: 0; batch classifier loss: 0.331367; batch adversarial loss: 0.727551\n",
      "epoch 1; iter: 200; batch classifier loss: 0.376800; batch adversarial loss: 0.668928\n",
      "epoch 2; iter: 0; batch classifier loss: 0.383275; batch adversarial loss: 0.655885\n",
      "epoch 2; iter: 200; batch classifier loss: 0.354992; batch adversarial loss: 0.666180\n",
      "epoch 3; iter: 0; batch classifier loss: 0.387695; batch adversarial loss: 0.642416\n",
      "epoch 3; iter: 200; batch classifier loss: 0.343166; batch adversarial loss: 0.603748\n",
      "epoch 4; iter: 0; batch classifier loss: 0.305611; batch adversarial loss: 0.639370\n",
      "epoch 4; iter: 200; batch classifier loss: 0.385529; batch adversarial loss: 0.626951\n",
      "epoch 5; iter: 0; batch classifier loss: 0.340520; batch adversarial loss: 0.634742\n",
      "epoch 5; iter: 200; batch classifier loss: 0.390288; batch adversarial loss: 0.601953\n",
      "epoch 6; iter: 0; batch classifier loss: 0.316183; batch adversarial loss: 0.651273\n",
      "epoch 6; iter: 200; batch classifier loss: 0.376272; batch adversarial loss: 0.557453\n",
      "epoch 7; iter: 0; batch classifier loss: 0.275434; batch adversarial loss: 0.604732\n",
      "epoch 7; iter: 200; batch classifier loss: 0.399105; batch adversarial loss: 0.598140\n",
      "epoch 8; iter: 0; batch classifier loss: 0.306047; batch adversarial loss: 0.601542\n",
      "epoch 8; iter: 200; batch classifier loss: 0.362329; batch adversarial loss: 0.633620\n",
      "epoch 9; iter: 0; batch classifier loss: 0.480388; batch adversarial loss: 0.652626\n",
      "epoch 9; iter: 200; batch classifier loss: 0.357457; batch adversarial loss: 0.638699\n",
      "epoch 10; iter: 0; batch classifier loss: 0.343500; batch adversarial loss: 0.641876\n",
      "epoch 10; iter: 200; batch classifier loss: 0.299207; batch adversarial loss: 0.602239\n",
      "epoch 11; iter: 0; batch classifier loss: 0.293219; batch adversarial loss: 0.649844\n",
      "epoch 11; iter: 200; batch classifier loss: 0.378841; batch adversarial loss: 0.656903\n",
      "epoch 12; iter: 0; batch classifier loss: 0.300507; batch adversarial loss: 0.654914\n",
      "epoch 12; iter: 200; batch classifier loss: 0.292534; batch adversarial loss: 0.594501\n",
      "epoch 13; iter: 0; batch classifier loss: 0.338410; batch adversarial loss: 0.559317\n",
      "epoch 13; iter: 200; batch classifier loss: 0.355023; batch adversarial loss: 0.630386\n",
      "epoch 14; iter: 0; batch classifier loss: 0.296017; batch adversarial loss: 0.625112\n",
      "epoch 14; iter: 200; batch classifier loss: 0.324068; batch adversarial loss: 0.638338\n",
      "epoch 15; iter: 0; batch classifier loss: 0.314696; batch adversarial loss: 0.635255\n",
      "epoch 15; iter: 200; batch classifier loss: 0.375089; batch adversarial loss: 0.669978\n",
      "epoch 16; iter: 0; batch classifier loss: 0.301578; batch adversarial loss: 0.603771\n",
      "epoch 16; iter: 200; batch classifier loss: 0.333869; batch adversarial loss: 0.615965\n",
      "epoch 17; iter: 0; batch classifier loss: 0.370861; batch adversarial loss: 0.618631\n",
      "epoch 17; iter: 200; batch classifier loss: 0.425697; batch adversarial loss: 0.603221\n",
      "epoch 18; iter: 0; batch classifier loss: 0.288897; batch adversarial loss: 0.567752\n",
      "epoch 18; iter: 200; batch classifier loss: 0.261907; batch adversarial loss: 0.627472\n",
      "epoch 19; iter: 0; batch classifier loss: 0.290709; batch adversarial loss: 0.574874\n",
      "epoch 19; iter: 200; batch classifier loss: 0.315964; batch adversarial loss: 0.645972\n",
      "epoch 20; iter: 0; batch classifier loss: 0.406819; batch adversarial loss: 0.621353\n",
      "epoch 20; iter: 200; batch classifier loss: 0.393686; batch adversarial loss: 0.625675\n",
      "epoch 21; iter: 0; batch classifier loss: 0.315938; batch adversarial loss: 0.646202\n",
      "epoch 21; iter: 200; batch classifier loss: 0.366303; batch adversarial loss: 0.615683\n",
      "epoch 22; iter: 0; batch classifier loss: 0.310716; batch adversarial loss: 0.633949\n",
      "epoch 22; iter: 200; batch classifier loss: 0.360043; batch adversarial loss: 0.574303\n",
      "epoch 23; iter: 0; batch classifier loss: 0.460573; batch adversarial loss: 0.588574\n",
      "epoch 23; iter: 200; batch classifier loss: 0.403518; batch adversarial loss: 0.635237\n",
      "epoch 24; iter: 0; batch classifier loss: 0.341684; batch adversarial loss: 0.640582\n",
      "epoch 24; iter: 200; batch classifier loss: 0.319376; batch adversarial loss: 0.622771\n",
      "epoch 25; iter: 0; batch classifier loss: 0.366880; batch adversarial loss: 0.648489\n",
      "epoch 25; iter: 200; batch classifier loss: 0.315295; batch adversarial loss: 0.621604\n",
      "epoch 26; iter: 0; batch classifier loss: 0.427196; batch adversarial loss: 0.607665\n",
      "epoch 26; iter: 200; batch classifier loss: 0.282546; batch adversarial loss: 0.645531\n",
      "epoch 27; iter: 0; batch classifier loss: 0.279384; batch adversarial loss: 0.540744\n",
      "epoch 27; iter: 200; batch classifier loss: 0.283778; batch adversarial loss: 0.597639\n",
      "epoch 28; iter: 0; batch classifier loss: 0.296992; batch adversarial loss: 0.616904\n",
      "epoch 28; iter: 200; batch classifier loss: 0.393431; batch adversarial loss: 0.600892\n",
      "epoch 29; iter: 0; batch classifier loss: 0.309356; batch adversarial loss: 0.603523\n",
      "epoch 29; iter: 200; batch classifier loss: 0.336620; batch adversarial loss: 0.620064\n",
      "epoch 30; iter: 0; batch classifier loss: 0.390149; batch adversarial loss: 0.555868\n",
      "epoch 30; iter: 200; batch classifier loss: 0.339937; batch adversarial loss: 0.609862\n",
      "epoch 31; iter: 0; batch classifier loss: 0.266992; batch adversarial loss: 0.553603\n",
      "epoch 31; iter: 200; batch classifier loss: 0.329127; batch adversarial loss: 0.675816\n",
      "epoch 32; iter: 0; batch classifier loss: 0.322773; batch adversarial loss: 0.639789\n",
      "epoch 32; iter: 200; batch classifier loss: 0.323358; batch adversarial loss: 0.608118\n",
      "epoch 33; iter: 0; batch classifier loss: 0.379588; batch adversarial loss: 0.604579\n",
      "epoch 33; iter: 200; batch classifier loss: 0.379116; batch adversarial loss: 0.612466\n",
      "epoch 34; iter: 0; batch classifier loss: 0.293338; batch adversarial loss: 0.615847\n",
      "epoch 34; iter: 200; batch classifier loss: 0.261502; batch adversarial loss: 0.655757\n",
      "epoch 35; iter: 0; batch classifier loss: 0.330998; batch adversarial loss: 0.595343\n",
      "epoch 35; iter: 200; batch classifier loss: 0.328319; batch adversarial loss: 0.626432\n",
      "epoch 36; iter: 0; batch classifier loss: 0.240102; batch adversarial loss: 0.634732\n",
      "epoch 36; iter: 200; batch classifier loss: 0.322690; batch adversarial loss: 0.644587\n",
      "epoch 37; iter: 0; batch classifier loss: 0.314061; batch adversarial loss: 0.636019\n",
      "epoch 37; iter: 200; batch classifier loss: 0.415404; batch adversarial loss: 0.633041\n",
      "epoch 38; iter: 0; batch classifier loss: 0.347704; batch adversarial loss: 0.630882\n",
      "epoch 38; iter: 200; batch classifier loss: 0.282680; batch adversarial loss: 0.669766\n",
      "epoch 39; iter: 0; batch classifier loss: 0.326771; batch adversarial loss: 0.580765\n",
      "epoch 39; iter: 200; batch classifier loss: 0.343974; batch adversarial loss: 0.634552\n",
      "epoch 40; iter: 0; batch classifier loss: 0.220226; batch adversarial loss: 0.641101\n",
      "epoch 40; iter: 200; batch classifier loss: 0.287280; batch adversarial loss: 0.660827\n",
      "epoch 41; iter: 0; batch classifier loss: 0.326987; batch adversarial loss: 0.609408\n",
      "epoch 41; iter: 200; batch classifier loss: 0.324972; batch adversarial loss: 0.624489\n",
      "epoch 42; iter: 0; batch classifier loss: 0.346165; batch adversarial loss: 0.618230\n",
      "epoch 42; iter: 200; batch classifier loss: 0.425624; batch adversarial loss: 0.568570\n",
      "epoch 43; iter: 0; batch classifier loss: 0.316288; batch adversarial loss: 0.631007\n",
      "epoch 43; iter: 200; batch classifier loss: 0.395885; batch adversarial loss: 0.611161\n",
      "epoch 44; iter: 0; batch classifier loss: 0.317920; batch adversarial loss: 0.589397\n",
      "epoch 44; iter: 200; batch classifier loss: 0.240431; batch adversarial loss: 0.634411\n",
      "epoch 45; iter: 0; batch classifier loss: 0.269169; batch adversarial loss: 0.664042\n",
      "epoch 45; iter: 200; batch classifier loss: 0.309336; batch adversarial loss: 0.654812\n",
      "epoch 46; iter: 0; batch classifier loss: 0.307167; batch adversarial loss: 0.638994\n",
      "epoch 46; iter: 200; batch classifier loss: 0.382942; batch adversarial loss: 0.577809\n",
      "epoch 47; iter: 0; batch classifier loss: 0.429777; batch adversarial loss: 0.610170\n",
      "epoch 47; iter: 200; batch classifier loss: 0.336463; batch adversarial loss: 0.575721\n",
      "epoch 48; iter: 0; batch classifier loss: 0.330312; batch adversarial loss: 0.623921\n",
      "epoch 48; iter: 200; batch classifier loss: 0.303247; batch adversarial loss: 0.648111\n",
      "epoch 49; iter: 0; batch classifier loss: 0.336950; batch adversarial loss: 0.617223\n",
      "epoch 49; iter: 200; batch classifier loss: 0.196543; batch adversarial loss: 0.654854\n"
     ]
    }
   ],
   "source": [
    "predicted_test_inprocessing = adversarialDebiasingProcessing(\n",
    "    train_aif_df=df_train_aif,  # Datos de entrenamiento\n",
    "    test_aif_df=df_test_aif,    # Datos de prueba\n",
    "    sensitive_features=['age', 'sex'],  # Atributos sensibles\n",
    "    num_epochs=50,  # Configuración del número de épocas\n",
    "    batch_size=128,  # Tamaño de lote\n",
    "    adversary_loss_weight=0.1,  # Peso del adversario\n",
    "    name_run_identifier='test_1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9UG6EWE2Pv8"
   },
   "source": [
    "## Post-procesamiento: **Equalized Odds Post-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "fPvO1_bP2T7_"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "#Código Post-Procesamiento\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "# Cramos una función que se encargue de reajustar las predicciones\n",
    "def eqOddsPredictionProccesing(\n",
    "    test_aif_df: BinaryLabelDataset,\n",
    "    test_pred_aif_df: BinaryLabelDataset,\n",
    "    sensitive_features: List[str],\n",
    "):\n",
    "\n",
    "    eq_odds_processers = []\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        eq_odds_processers.append(\n",
    "            EqOddsPostprocessing(\n",
    "                unprivileged_groups=[{sensitive_feature: 0}],\n",
    "                privileged_groups=[{sensitive_feature: 1}],\n",
    "                seed=42\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    actual_pred_aif_df = test_pred_aif_df.copy()\n",
    "    \n",
    "    for eq_odds_processer in eq_odds_processers:\n",
    "        eq_odds_processer: EqOddsPostprocessing\n",
    "        eq_odds_processer.fit(test_aif_df, actual_pred_aif_df)\n",
    "        actual_pred_aif_df = eq_odds_processer.predict(actual_pred_aif_df)\n",
    "        \n",
    "    return actual_pred_aif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.0650931315940388\n",
      "Disparate Impact for sex: 0.6202328376320687\n",
      "New Accuracy: 0.8115069643732065\n"
     ]
    }
   ],
   "source": [
    "post_processed_preds = eqOddsPredictionProccesing(\n",
    "    df_test_aif,\n",
    "    df_test_pred_aif, \n",
    "    ['age', 'sex']\n",
    ")\n",
    "\n",
    "get_demographic_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds\n",
    ")\n",
    "\n",
    "new_accuracy = accuracy_score(y_test, post_processed_preds.labels)\n",
    "print(f\"New Accuracy: {new_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHcG02a8lt4W"
   },
   "source": [
    "# Medición de Mitigación de Sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRQWwinZ2Yij"
   },
   "source": [
    "## Combinación 1: Pre-procesamiento + In-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "czYk1bQ80dx9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo debiasado para age...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.654990; batch adversarial loss: 0.544777\n",
      "epoch 0; iter: 200; batch classifier loss: 0.380507; batch adversarial loss: 0.603652\n",
      "epoch 1; iter: 0; batch classifier loss: 0.395995; batch adversarial loss: 0.580569\n",
      "epoch 1; iter: 200; batch classifier loss: 0.538597; batch adversarial loss: 0.527204\n",
      "epoch 2; iter: 0; batch classifier loss: 0.529805; batch adversarial loss: 0.499234\n",
      "epoch 2; iter: 200; batch classifier loss: 0.417079; batch adversarial loss: 0.425837\n",
      "epoch 3; iter: 0; batch classifier loss: 0.394124; batch adversarial loss: 0.486551\n",
      "epoch 3; iter: 200; batch classifier loss: 0.359398; batch adversarial loss: 0.429863\n",
      "epoch 4; iter: 0; batch classifier loss: 0.398436; batch adversarial loss: 0.446331\n",
      "epoch 4; iter: 200; batch classifier loss: 0.476476; batch adversarial loss: 0.404803\n",
      "epoch 5; iter: 0; batch classifier loss: 0.432306; batch adversarial loss: 0.375442\n",
      "epoch 5; iter: 200; batch classifier loss: 0.396975; batch adversarial loss: 0.354928\n",
      "epoch 6; iter: 0; batch classifier loss: 0.358270; batch adversarial loss: 0.359704\n",
      "epoch 6; iter: 200; batch classifier loss: 0.316217; batch adversarial loss: 0.307878\n",
      "epoch 7; iter: 0; batch classifier loss: 0.378614; batch adversarial loss: 0.280605\n",
      "epoch 7; iter: 200; batch classifier loss: 0.375069; batch adversarial loss: 0.304576\n",
      "epoch 8; iter: 0; batch classifier loss: 0.320749; batch adversarial loss: 0.376515\n",
      "epoch 8; iter: 200; batch classifier loss: 0.351684; batch adversarial loss: 0.296891\n",
      "epoch 9; iter: 0; batch classifier loss: 0.363345; batch adversarial loss: 0.339473\n",
      "epoch 9; iter: 200; batch classifier loss: 0.377206; batch adversarial loss: 0.272381\n",
      "epoch 10; iter: 0; batch classifier loss: 0.211999; batch adversarial loss: 0.240641\n",
      "epoch 10; iter: 200; batch classifier loss: 0.332523; batch adversarial loss: 0.331501\n",
      "epoch 11; iter: 0; batch classifier loss: 0.363742; batch adversarial loss: 0.213575\n",
      "epoch 11; iter: 200; batch classifier loss: 0.410944; batch adversarial loss: 0.248033\n",
      "epoch 12; iter: 0; batch classifier loss: 0.255769; batch adversarial loss: 0.358789\n",
      "epoch 12; iter: 200; batch classifier loss: 0.360807; batch adversarial loss: 0.283496\n",
      "epoch 13; iter: 0; batch classifier loss: 0.249208; batch adversarial loss: 0.255083\n",
      "epoch 13; iter: 200; batch classifier loss: 0.299677; batch adversarial loss: 0.291697\n",
      "epoch 14; iter: 0; batch classifier loss: 0.334248; batch adversarial loss: 0.297689\n",
      "epoch 14; iter: 200; batch classifier loss: 0.362880; batch adversarial loss: 0.293606\n",
      "epoch 15; iter: 0; batch classifier loss: 0.282897; batch adversarial loss: 0.320894\n",
      "epoch 15; iter: 200; batch classifier loss: 0.298492; batch adversarial loss: 0.346602\n",
      "epoch 16; iter: 0; batch classifier loss: 0.301340; batch adversarial loss: 0.319640\n",
      "epoch 16; iter: 200; batch classifier loss: 0.392600; batch adversarial loss: 0.386320\n",
      "epoch 17; iter: 0; batch classifier loss: 0.302268; batch adversarial loss: 0.296022\n",
      "epoch 17; iter: 200; batch classifier loss: 0.257159; batch adversarial loss: 0.317806\n",
      "epoch 18; iter: 0; batch classifier loss: 0.348656; batch adversarial loss: 0.255997\n",
      "epoch 18; iter: 200; batch classifier loss: 0.300856; batch adversarial loss: 0.235184\n",
      "epoch 19; iter: 0; batch classifier loss: 0.319438; batch adversarial loss: 0.236591\n",
      "epoch 19; iter: 200; batch classifier loss: 0.376907; batch adversarial loss: 0.392639\n",
      "epoch 20; iter: 0; batch classifier loss: 0.253215; batch adversarial loss: 0.325485\n",
      "epoch 20; iter: 200; batch classifier loss: 0.260396; batch adversarial loss: 0.351593\n",
      "epoch 21; iter: 0; batch classifier loss: 0.219417; batch adversarial loss: 0.203878\n",
      "epoch 21; iter: 200; batch classifier loss: 0.230811; batch adversarial loss: 0.256796\n",
      "epoch 22; iter: 0; batch classifier loss: 0.332432; batch adversarial loss: 0.216854\n",
      "epoch 22; iter: 200; batch classifier loss: 0.256679; batch adversarial loss: 0.219349\n",
      "epoch 23; iter: 0; batch classifier loss: 0.325124; batch adversarial loss: 0.348732\n",
      "epoch 23; iter: 200; batch classifier loss: 0.287250; batch adversarial loss: 0.295137\n",
      "epoch 24; iter: 0; batch classifier loss: 0.280456; batch adversarial loss: 0.255207\n",
      "epoch 24; iter: 200; batch classifier loss: 0.271122; batch adversarial loss: 0.275206\n",
      "epoch 25; iter: 0; batch classifier loss: 0.337356; batch adversarial loss: 0.275180\n",
      "epoch 25; iter: 200; batch classifier loss: 0.364427; batch adversarial loss: 0.273957\n",
      "epoch 26; iter: 0; batch classifier loss: 0.250504; batch adversarial loss: 0.330404\n",
      "epoch 26; iter: 200; batch classifier loss: 0.274077; batch adversarial loss: 0.217966\n",
      "epoch 27; iter: 0; batch classifier loss: 0.291511; batch adversarial loss: 0.331189\n",
      "epoch 27; iter: 200; batch classifier loss: 0.319610; batch adversarial loss: 0.311914\n",
      "epoch 28; iter: 0; batch classifier loss: 0.332472; batch adversarial loss: 0.273609\n",
      "epoch 28; iter: 200; batch classifier loss: 0.300106; batch adversarial loss: 0.254585\n",
      "epoch 29; iter: 0; batch classifier loss: 0.209458; batch adversarial loss: 0.179498\n",
      "epoch 29; iter: 200; batch classifier loss: 0.431536; batch adversarial loss: 0.274529\n",
      "epoch 30; iter: 0; batch classifier loss: 0.342234; batch adversarial loss: 0.254872\n",
      "epoch 30; iter: 200; batch classifier loss: 0.333834; batch adversarial loss: 0.236269\n",
      "epoch 31; iter: 0; batch classifier loss: 0.256031; batch adversarial loss: 0.445465\n",
      "epoch 31; iter: 200; batch classifier loss: 0.313824; batch adversarial loss: 0.350505\n",
      "epoch 32; iter: 0; batch classifier loss: 0.263838; batch adversarial loss: 0.199058\n",
      "epoch 32; iter: 200; batch classifier loss: 0.331813; batch adversarial loss: 0.254153\n",
      "epoch 33; iter: 0; batch classifier loss: 0.277034; batch adversarial loss: 0.313793\n",
      "epoch 33; iter: 200; batch classifier loss: 0.317070; batch adversarial loss: 0.198997\n",
      "epoch 34; iter: 0; batch classifier loss: 0.342206; batch adversarial loss: 0.255361\n",
      "epoch 34; iter: 200; batch classifier loss: 0.266340; batch adversarial loss: 0.235692\n",
      "epoch 35; iter: 0; batch classifier loss: 0.236772; batch adversarial loss: 0.216734\n",
      "epoch 35; iter: 200; batch classifier loss: 0.327777; batch adversarial loss: 0.217354\n",
      "epoch 36; iter: 0; batch classifier loss: 0.213875; batch adversarial loss: 0.255729\n",
      "epoch 36; iter: 200; batch classifier loss: 0.244407; batch adversarial loss: 0.236363\n",
      "epoch 37; iter: 0; batch classifier loss: 0.261335; batch adversarial loss: 0.236015\n",
      "epoch 37; iter: 200; batch classifier loss: 0.328355; batch adversarial loss: 0.273588\n",
      "epoch 38; iter: 0; batch classifier loss: 0.356367; batch adversarial loss: 0.235880\n",
      "epoch 38; iter: 200; batch classifier loss: 0.304006; batch adversarial loss: 0.274379\n",
      "epoch 39; iter: 0; batch classifier loss: 0.366873; batch adversarial loss: 0.292991\n",
      "epoch 39; iter: 200; batch classifier loss: 0.313417; batch adversarial loss: 0.388875\n",
      "epoch 40; iter: 0; batch classifier loss: 0.376685; batch adversarial loss: 0.254971\n",
      "epoch 40; iter: 200; batch classifier loss: 0.236857; batch adversarial loss: 0.235729\n",
      "epoch 41; iter: 0; batch classifier loss: 0.341083; batch adversarial loss: 0.236466\n",
      "epoch 41; iter: 200; batch classifier loss: 0.288010; batch adversarial loss: 0.312049\n",
      "epoch 42; iter: 0; batch classifier loss: 0.280054; batch adversarial loss: 0.235974\n",
      "epoch 42; iter: 200; batch classifier loss: 0.381161; batch adversarial loss: 0.236341\n",
      "epoch 43; iter: 0; batch classifier loss: 0.261859; batch adversarial loss: 0.425949\n",
      "epoch 43; iter: 200; batch classifier loss: 0.283074; batch adversarial loss: 0.255514\n",
      "epoch 44; iter: 0; batch classifier loss: 0.243022; batch adversarial loss: 0.331454\n",
      "epoch 44; iter: 200; batch classifier loss: 0.211919; batch adversarial loss: 0.312483\n",
      "epoch 45; iter: 0; batch classifier loss: 0.322284; batch adversarial loss: 0.217249\n",
      "epoch 45; iter: 200; batch classifier loss: 0.266160; batch adversarial loss: 0.292762\n",
      "epoch 46; iter: 0; batch classifier loss: 0.296461; batch adversarial loss: 0.426928\n",
      "epoch 46; iter: 200; batch classifier loss: 0.238835; batch adversarial loss: 0.331432\n",
      "epoch 47; iter: 0; batch classifier loss: 0.360718; batch adversarial loss: 0.255505\n",
      "epoch 47; iter: 200; batch classifier loss: 0.280908; batch adversarial loss: 0.350452\n",
      "epoch 48; iter: 0; batch classifier loss: 0.242946; batch adversarial loss: 0.331395\n",
      "epoch 48; iter: 200; batch classifier loss: 0.242559; batch adversarial loss: 0.235963\n",
      "epoch 49; iter: 0; batch classifier loss: 0.229552; batch adversarial loss: 0.293184\n",
      "epoch 49; iter: 200; batch classifier loss: 0.263753; batch adversarial loss: 0.255251\n",
      "Entrenando modelo debiasado para sex...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.710084; batch adversarial loss: 0.642355\n",
      "epoch 0; iter: 200; batch classifier loss: 0.510018; batch adversarial loss: 0.653532\n",
      "epoch 1; iter: 0; batch classifier loss: 0.358908; batch adversarial loss: 0.661956\n",
      "epoch 1; iter: 200; batch classifier loss: 0.317806; batch adversarial loss: 0.653954\n",
      "epoch 2; iter: 0; batch classifier loss: 0.475078; batch adversarial loss: 0.647910\n",
      "epoch 2; iter: 200; batch classifier loss: 0.373956; batch adversarial loss: 0.618156\n",
      "epoch 3; iter: 0; batch classifier loss: 0.321490; batch adversarial loss: 0.619287\n",
      "epoch 3; iter: 200; batch classifier loss: 0.358899; batch adversarial loss: 0.659953\n",
      "epoch 4; iter: 0; batch classifier loss: 0.405759; batch adversarial loss: 0.614462\n",
      "epoch 4; iter: 200; batch classifier loss: 0.319931; batch adversarial loss: 0.638115\n",
      "epoch 5; iter: 0; batch classifier loss: 0.270897; batch adversarial loss: 0.665857\n",
      "epoch 5; iter: 200; batch classifier loss: 0.339766; batch adversarial loss: 0.636620\n",
      "epoch 6; iter: 0; batch classifier loss: 0.274157; batch adversarial loss: 0.687756\n",
      "epoch 6; iter: 200; batch classifier loss: 0.303258; batch adversarial loss: 0.609124\n",
      "epoch 7; iter: 0; batch classifier loss: 0.374586; batch adversarial loss: 0.586396\n",
      "epoch 7; iter: 200; batch classifier loss: 0.268261; batch adversarial loss: 0.574530\n",
      "epoch 8; iter: 0; batch classifier loss: 0.363963; batch adversarial loss: 0.640604\n",
      "epoch 8; iter: 200; batch classifier loss: 0.304654; batch adversarial loss: 0.684855\n",
      "epoch 9; iter: 0; batch classifier loss: 0.431648; batch adversarial loss: 0.594742\n",
      "epoch 9; iter: 200; batch classifier loss: 0.285657; batch adversarial loss: 0.641531\n",
      "epoch 10; iter: 0; batch classifier loss: 0.318853; batch adversarial loss: 0.634838\n",
      "epoch 10; iter: 200; batch classifier loss: 0.291931; batch adversarial loss: 0.669934\n",
      "epoch 11; iter: 0; batch classifier loss: 0.355386; batch adversarial loss: 0.651468\n",
      "epoch 11; iter: 200; batch classifier loss: 0.358742; batch adversarial loss: 0.641742\n",
      "epoch 12; iter: 0; batch classifier loss: 0.285190; batch adversarial loss: 0.662356\n",
      "epoch 12; iter: 200; batch classifier loss: 0.397697; batch adversarial loss: 0.615029\n",
      "epoch 13; iter: 0; batch classifier loss: 0.371385; batch adversarial loss: 0.615117\n",
      "epoch 13; iter: 200; batch classifier loss: 0.337044; batch adversarial loss: 0.642984\n",
      "epoch 14; iter: 0; batch classifier loss: 0.320630; batch adversarial loss: 0.637110\n",
      "epoch 14; iter: 200; batch classifier loss: 0.318219; batch adversarial loss: 0.663572\n",
      "epoch 15; iter: 0; batch classifier loss: 0.295104; batch adversarial loss: 0.582581\n",
      "epoch 15; iter: 200; batch classifier loss: 0.298304; batch adversarial loss: 0.622699\n",
      "epoch 16; iter: 0; batch classifier loss: 0.305777; batch adversarial loss: 0.594520\n",
      "epoch 16; iter: 200; batch classifier loss: 0.320617; batch adversarial loss: 0.607182\n",
      "epoch 17; iter: 0; batch classifier loss: 0.328509; batch adversarial loss: 0.525116\n",
      "epoch 17; iter: 200; batch classifier loss: 0.319638; batch adversarial loss: 0.571847\n",
      "epoch 18; iter: 0; batch classifier loss: 0.362500; batch adversarial loss: 0.614409\n",
      "epoch 18; iter: 200; batch classifier loss: 0.307409; batch adversarial loss: 0.601031\n",
      "epoch 19; iter: 0; batch classifier loss: 0.288286; batch adversarial loss: 0.658481\n",
      "epoch 19; iter: 200; batch classifier loss: 0.254883; batch adversarial loss: 0.601533\n",
      "epoch 20; iter: 0; batch classifier loss: 0.344733; batch adversarial loss: 0.663730\n",
      "epoch 20; iter: 200; batch classifier loss: 0.282294; batch adversarial loss: 0.685072\n",
      "epoch 21; iter: 0; batch classifier loss: 0.307284; batch adversarial loss: 0.610217\n",
      "epoch 21; iter: 200; batch classifier loss: 0.380308; batch adversarial loss: 0.635178\n",
      "epoch 22; iter: 0; batch classifier loss: 0.264998; batch adversarial loss: 0.626700\n",
      "epoch 22; iter: 200; batch classifier loss: 0.366576; batch adversarial loss: 0.664175\n",
      "epoch 23; iter: 0; batch classifier loss: 0.343573; batch adversarial loss: 0.608455\n",
      "epoch 23; iter: 200; batch classifier loss: 0.329578; batch adversarial loss: 0.589899\n",
      "epoch 24; iter: 0; batch classifier loss: 0.350840; batch adversarial loss: 0.636421\n",
      "epoch 24; iter: 200; batch classifier loss: 0.286909; batch adversarial loss: 0.585529\n",
      "epoch 25; iter: 0; batch classifier loss: 0.328338; batch adversarial loss: 0.630825\n",
      "epoch 25; iter: 200; batch classifier loss: 0.381497; batch adversarial loss: 0.593831\n",
      "epoch 26; iter: 0; batch classifier loss: 0.328103; batch adversarial loss: 0.665311\n",
      "epoch 26; iter: 200; batch classifier loss: 0.344827; batch adversarial loss: 0.645128\n",
      "epoch 27; iter: 0; batch classifier loss: 0.326966; batch adversarial loss: 0.655653\n",
      "epoch 27; iter: 200; batch classifier loss: 0.370563; batch adversarial loss: 0.578172\n",
      "epoch 28; iter: 0; batch classifier loss: 0.323070; batch adversarial loss: 0.712023\n",
      "epoch 28; iter: 200; batch classifier loss: 0.334104; batch adversarial loss: 0.571902\n",
      "epoch 29; iter: 0; batch classifier loss: 0.307630; batch adversarial loss: 0.613211\n",
      "epoch 29; iter: 200; batch classifier loss: 0.390597; batch adversarial loss: 0.617263\n",
      "epoch 30; iter: 0; batch classifier loss: 0.225092; batch adversarial loss: 0.699789\n",
      "epoch 30; iter: 200; batch classifier loss: 0.394038; batch adversarial loss: 0.612609\n",
      "epoch 31; iter: 0; batch classifier loss: 0.322072; batch adversarial loss: 0.608813\n",
      "epoch 31; iter: 200; batch classifier loss: 0.266681; batch adversarial loss: 0.681397\n",
      "epoch 32; iter: 0; batch classifier loss: 0.344412; batch adversarial loss: 0.614947\n",
      "epoch 32; iter: 200; batch classifier loss: 0.281581; batch adversarial loss: 0.583375\n",
      "epoch 33; iter: 0; batch classifier loss: 0.342815; batch adversarial loss: 0.648963\n",
      "epoch 33; iter: 200; batch classifier loss: 0.383831; batch adversarial loss: 0.574253\n",
      "epoch 34; iter: 0; batch classifier loss: 0.344665; batch adversarial loss: 0.620350\n",
      "epoch 34; iter: 200; batch classifier loss: 0.376680; batch adversarial loss: 0.601277\n",
      "epoch 35; iter: 0; batch classifier loss: 0.337255; batch adversarial loss: 0.534675\n",
      "epoch 35; iter: 200; batch classifier loss: 0.426850; batch adversarial loss: 0.587764\n",
      "epoch 36; iter: 0; batch classifier loss: 0.327614; batch adversarial loss: 0.620910\n",
      "epoch 36; iter: 200; batch classifier loss: 0.331486; batch adversarial loss: 0.633901\n",
      "epoch 37; iter: 0; batch classifier loss: 0.305425; batch adversarial loss: 0.665112\n",
      "epoch 37; iter: 200; batch classifier loss: 0.378842; batch adversarial loss: 0.640481\n",
      "epoch 38; iter: 0; batch classifier loss: 0.273294; batch adversarial loss: 0.662432\n",
      "epoch 38; iter: 200; batch classifier loss: 0.393070; batch adversarial loss: 0.640663\n",
      "epoch 39; iter: 0; batch classifier loss: 0.328657; batch adversarial loss: 0.618140\n",
      "epoch 39; iter: 200; batch classifier loss: 0.455122; batch adversarial loss: 0.603326\n",
      "epoch 40; iter: 0; batch classifier loss: 0.316370; batch adversarial loss: 0.580470\n",
      "epoch 40; iter: 200; batch classifier loss: 0.363505; batch adversarial loss: 0.619827\n",
      "epoch 41; iter: 0; batch classifier loss: 0.222469; batch adversarial loss: 0.683552\n",
      "epoch 41; iter: 200; batch classifier loss: 0.339492; batch adversarial loss: 0.627762\n",
      "epoch 42; iter: 0; batch classifier loss: 0.239971; batch adversarial loss: 0.662603\n",
      "epoch 42; iter: 200; batch classifier loss: 0.326666; batch adversarial loss: 0.648779\n",
      "epoch 43; iter: 0; batch classifier loss: 0.311595; batch adversarial loss: 0.634515\n",
      "epoch 43; iter: 200; batch classifier loss: 0.351209; batch adversarial loss: 0.574413\n",
      "epoch 44; iter: 0; batch classifier loss: 0.327142; batch adversarial loss: 0.570950\n",
      "epoch 44; iter: 200; batch classifier loss: 0.318413; batch adversarial loss: 0.578391\n",
      "epoch 45; iter: 0; batch classifier loss: 0.319049; batch adversarial loss: 0.548133\n",
      "epoch 45; iter: 200; batch classifier loss: 0.291293; batch adversarial loss: 0.597793\n",
      "epoch 46; iter: 0; batch classifier loss: 0.314639; batch adversarial loss: 0.567289\n",
      "epoch 46; iter: 200; batch classifier loss: 0.256046; batch adversarial loss: 0.689149\n",
      "epoch 47; iter: 0; batch classifier loss: 0.330181; batch adversarial loss: 0.589102\n",
      "epoch 47; iter: 200; batch classifier loss: 0.393303; batch adversarial loss: 0.668592\n",
      "epoch 48; iter: 0; batch classifier loss: 0.336239; batch adversarial loss: 0.639390\n",
      "epoch 48; iter: 200; batch classifier loss: 0.378384; batch adversarial loss: 0.623668\n",
      "epoch 49; iter: 0; batch classifier loss: 0.285578; batch adversarial loss: 0.617525\n",
      "epoch 49; iter: 200; batch classifier loss: 0.283118; batch adversarial loss: 0.593134\n"
     ]
    }
   ],
   "source": [
    "## Código que combine las dos técnicas\n",
    "reweighted_train = reweighingPreprocessing(\n",
    "    df_train_aif,\n",
    "    ['age', 'sex']\n",
    ")\n",
    "\n",
    "predicted_test_inprocessing_1 = adversarialDebiasingProcessing(\n",
    "    train_aif_df=reweighted_train,  # Datos de entrenamiento\n",
    "    test_aif_df=df_test_aif,    # Datos de prueba\n",
    "    sensitive_features=['age', 'sex'],  # Atributos sensibles\n",
    "    num_epochs=50,  # Configuración del número de épocas\n",
    "    batch_size=128,  # Tamaño de lote\n",
    "    adversary_loss_weight=0.1,  # Peso del adversario\n",
    "    name_run_identifier='test_2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tywIOpMw0l4k"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.0136371951791556\n",
      "Disparate Impact for sex: 0.9397440669775219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<aif360.metrics.classification_metric.ClassificationMetric at 0x1655bfecbf0>,\n",
       " <aif360.metrics.classification_metric.ClassificationMetric at 0x1655bc97ef0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_demographic_parity_metrics(\n",
    "    df_test_aif,\n",
    "    predicted_test_inprocessing_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7jAqwb8l6XB"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Older Adults\n",
      "  True Positive Rate (TPR): 0.47\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Age Group: Young Adults\n",
      "  True Positive Rate (TPR): 0.51\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Sex Group: Female\n",
      "  True Positive Rate (TPR): 0.74\n",
      "  False Positive Rate (FPR): 0.09\n",
      "Sex Group: Male\n",
      "  True Positive Rate (TPR): 0.46\n",
      "  False Positive Rate (FPR): 0.05\n"
     ]
    }
   ],
   "source": [
    "get_equalized_odds_metrics(\n",
    "    X_test_df,\n",
    "    y_test,\n",
    "    predicted_test_inprocessing_1.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIWoDqa30mHI"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged PPV age: 0.7095782073813708\n",
      "Unprivileged PPV age: 0.7208121827411168\n",
      "Predictive Parity Difference age: -0.011233975359746018\n",
      "Privileged PPV sex: 0.7310683585755219\n",
      "Unprivileged PPV sex: 0.7130177514792899\n",
      "Predictive Parity Difference sex: 0.018050607096231963\n"
     ]
    }
   ],
   "source": [
    "get_predictive_parity_metrics(\n",
    "    df_test_aif,\n",
    "    predicted_test_inprocessing_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305452509274165"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predicted_test_inprocessing_1.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSQYOgL504SD"
   },
   "source": [
    "## Combinación 2: In-procesamiento + Post-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wyd97ICz083J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo debiasado para age...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.676101; batch adversarial loss: 0.731749\n",
      "epoch 0; iter: 200; batch classifier loss: 0.402378; batch adversarial loss: 0.622374\n",
      "epoch 1; iter: 0; batch classifier loss: 0.353756; batch adversarial loss: 0.594891\n",
      "epoch 1; iter: 200; batch classifier loss: 0.426250; batch adversarial loss: 0.519466\n",
      "epoch 2; iter: 0; batch classifier loss: 0.421696; batch adversarial loss: 0.518851\n",
      "epoch 2; iter: 200; batch classifier loss: 0.454373; batch adversarial loss: 0.471482\n",
      "epoch 3; iter: 0; batch classifier loss: 0.489997; batch adversarial loss: 0.454455\n",
      "epoch 3; iter: 200; batch classifier loss: 0.425716; batch adversarial loss: 0.446720\n",
      "epoch 4; iter: 0; batch classifier loss: 0.494585; batch adversarial loss: 0.391426\n",
      "epoch 4; iter: 200; batch classifier loss: 0.510633; batch adversarial loss: 0.326643\n",
      "epoch 5; iter: 0; batch classifier loss: 0.449365; batch adversarial loss: 0.388905\n",
      "epoch 5; iter: 200; batch classifier loss: 0.293138; batch adversarial loss: 0.351151\n",
      "epoch 6; iter: 0; batch classifier loss: 0.388217; batch adversarial loss: 0.355571\n",
      "epoch 6; iter: 200; batch classifier loss: 0.452212; batch adversarial loss: 0.312963\n",
      "epoch 7; iter: 0; batch classifier loss: 0.270609; batch adversarial loss: 0.266553\n",
      "epoch 7; iter: 200; batch classifier loss: 0.338422; batch adversarial loss: 0.259771\n",
      "epoch 8; iter: 0; batch classifier loss: 0.242081; batch adversarial loss: 0.378538\n",
      "epoch 8; iter: 200; batch classifier loss: 0.294834; batch adversarial loss: 0.327435\n",
      "epoch 9; iter: 0; batch classifier loss: 0.317223; batch adversarial loss: 0.340490\n",
      "epoch 9; iter: 200; batch classifier loss: 0.311872; batch adversarial loss: 0.315244\n",
      "epoch 10; iter: 0; batch classifier loss: 0.352275; batch adversarial loss: 0.261313\n",
      "epoch 10; iter: 200; batch classifier loss: 0.352335; batch adversarial loss: 0.356218\n",
      "epoch 11; iter: 0; batch classifier loss: 0.230647; batch adversarial loss: 0.231799\n",
      "epoch 11; iter: 200; batch classifier loss: 0.335387; batch adversarial loss: 0.266450\n",
      "epoch 12; iter: 0; batch classifier loss: 0.265447; batch adversarial loss: 0.226698\n",
      "epoch 12; iter: 200; batch classifier loss: 0.251501; batch adversarial loss: 0.235597\n",
      "epoch 13; iter: 0; batch classifier loss: 0.344627; batch adversarial loss: 0.278696\n",
      "epoch 13; iter: 200; batch classifier loss: 0.323634; batch adversarial loss: 0.232324\n",
      "epoch 14; iter: 0; batch classifier loss: 0.294321; batch adversarial loss: 0.281718\n",
      "epoch 14; iter: 200; batch classifier loss: 0.299216; batch adversarial loss: 0.310185\n",
      "epoch 15; iter: 0; batch classifier loss: 0.228858; batch adversarial loss: 0.281072\n",
      "epoch 15; iter: 200; batch classifier loss: 0.356359; batch adversarial loss: 0.298897\n",
      "epoch 16; iter: 0; batch classifier loss: 0.291904; batch adversarial loss: 0.222295\n",
      "epoch 16; iter: 200; batch classifier loss: 0.360153; batch adversarial loss: 0.197572\n",
      "epoch 17; iter: 0; batch classifier loss: 0.260137; batch adversarial loss: 0.220982\n",
      "epoch 17; iter: 200; batch classifier loss: 0.356407; batch adversarial loss: 0.347253\n",
      "epoch 18; iter: 0; batch classifier loss: 0.226709; batch adversarial loss: 0.259778\n",
      "epoch 18; iter: 200; batch classifier loss: 0.280706; batch adversarial loss: 0.313839\n",
      "epoch 19; iter: 0; batch classifier loss: 0.315471; batch adversarial loss: 0.297150\n",
      "epoch 19; iter: 200; batch classifier loss: 0.291236; batch adversarial loss: 0.199806\n",
      "epoch 20; iter: 0; batch classifier loss: 0.261537; batch adversarial loss: 0.238314\n",
      "epoch 20; iter: 200; batch classifier loss: 0.233456; batch adversarial loss: 0.254739\n",
      "epoch 21; iter: 0; batch classifier loss: 0.323646; batch adversarial loss: 0.146560\n",
      "epoch 21; iter: 200; batch classifier loss: 0.227272; batch adversarial loss: 0.236019\n",
      "epoch 22; iter: 0; batch classifier loss: 0.265390; batch adversarial loss: 0.275637\n",
      "epoch 22; iter: 200; batch classifier loss: 0.374513; batch adversarial loss: 0.274738\n",
      "epoch 23; iter: 0; batch classifier loss: 0.358967; batch adversarial loss: 0.386740\n",
      "epoch 23; iter: 200; batch classifier loss: 0.370515; batch adversarial loss: 0.331843\n",
      "epoch 24; iter: 0; batch classifier loss: 0.339026; batch adversarial loss: 0.238735\n",
      "epoch 24; iter: 200; batch classifier loss: 0.263671; batch adversarial loss: 0.273392\n",
      "epoch 25; iter: 0; batch classifier loss: 0.321928; batch adversarial loss: 0.292555\n",
      "epoch 25; iter: 200; batch classifier loss: 0.241490; batch adversarial loss: 0.198130\n",
      "epoch 26; iter: 0; batch classifier loss: 0.326824; batch adversarial loss: 0.256273\n",
      "epoch 26; iter: 200; batch classifier loss: 0.348129; batch adversarial loss: 0.350209\n",
      "epoch 27; iter: 0; batch classifier loss: 0.234747; batch adversarial loss: 0.329551\n",
      "epoch 27; iter: 200; batch classifier loss: 0.304745; batch adversarial loss: 0.274818\n",
      "epoch 28; iter: 0; batch classifier loss: 0.355959; batch adversarial loss: 0.254481\n",
      "epoch 28; iter: 200; batch classifier loss: 0.289378; batch adversarial loss: 0.216451\n",
      "epoch 29; iter: 0; batch classifier loss: 0.332403; batch adversarial loss: 0.197846\n",
      "epoch 29; iter: 200; batch classifier loss: 0.324693; batch adversarial loss: 0.292651\n",
      "epoch 30; iter: 0; batch classifier loss: 0.276411; batch adversarial loss: 0.350772\n",
      "epoch 30; iter: 200; batch classifier loss: 0.206995; batch adversarial loss: 0.331236\n",
      "epoch 31; iter: 0; batch classifier loss: 0.362894; batch adversarial loss: 0.217416\n",
      "epoch 31; iter: 200; batch classifier loss: 0.308117; batch adversarial loss: 0.370200\n",
      "epoch 32; iter: 0; batch classifier loss: 0.388533; batch adversarial loss: 0.312817\n",
      "epoch 32; iter: 200; batch classifier loss: 0.276382; batch adversarial loss: 0.255266\n",
      "epoch 33; iter: 0; batch classifier loss: 0.382709; batch adversarial loss: 0.312216\n",
      "epoch 33; iter: 200; batch classifier loss: 0.319666; batch adversarial loss: 0.236778\n",
      "epoch 34; iter: 0; batch classifier loss: 0.309746; batch adversarial loss: 0.369693\n",
      "epoch 34; iter: 200; batch classifier loss: 0.257965; batch adversarial loss: 0.255873\n",
      "epoch 35; iter: 0; batch classifier loss: 0.305773; batch adversarial loss: 0.274510\n",
      "epoch 35; iter: 200; batch classifier loss: 0.292514; batch adversarial loss: 0.293615\n",
      "epoch 36; iter: 0; batch classifier loss: 0.312558; batch adversarial loss: 0.236706\n",
      "epoch 36; iter: 200; batch classifier loss: 0.216439; batch adversarial loss: 0.369560\n",
      "epoch 37; iter: 0; batch classifier loss: 0.281339; batch adversarial loss: 0.387892\n",
      "epoch 37; iter: 200; batch classifier loss: 0.208038; batch adversarial loss: 0.255242\n",
      "epoch 38; iter: 0; batch classifier loss: 0.338307; batch adversarial loss: 0.349762\n",
      "epoch 38; iter: 200; batch classifier loss: 0.388865; batch adversarial loss: 0.273943\n",
      "epoch 39; iter: 0; batch classifier loss: 0.335149; batch adversarial loss: 0.217174\n",
      "epoch 39; iter: 200; batch classifier loss: 0.362186; batch adversarial loss: 0.236072\n",
      "epoch 40; iter: 0; batch classifier loss: 0.332962; batch adversarial loss: 0.350452\n",
      "epoch 40; iter: 200; batch classifier loss: 0.269775; batch adversarial loss: 0.274641\n",
      "epoch 41; iter: 0; batch classifier loss: 0.334015; batch adversarial loss: 0.273758\n",
      "epoch 41; iter: 200; batch classifier loss: 0.344481; batch adversarial loss: 0.331662\n",
      "epoch 42; iter: 0; batch classifier loss: 0.247399; batch adversarial loss: 0.274578\n",
      "epoch 42; iter: 200; batch classifier loss: 0.295950; batch adversarial loss: 0.369293\n",
      "epoch 43; iter: 0; batch classifier loss: 0.295241; batch adversarial loss: 0.217422\n",
      "epoch 43; iter: 200; batch classifier loss: 0.265207; batch adversarial loss: 0.274212\n",
      "epoch 44; iter: 0; batch classifier loss: 0.272389; batch adversarial loss: 0.236341\n",
      "epoch 44; iter: 200; batch classifier loss: 0.320167; batch adversarial loss: 0.331136\n",
      "epoch 45; iter: 0; batch classifier loss: 0.367402; batch adversarial loss: 0.197869\n",
      "epoch 45; iter: 200; batch classifier loss: 0.291715; batch adversarial loss: 0.351014\n",
      "epoch 46; iter: 0; batch classifier loss: 0.279145; batch adversarial loss: 0.331140\n",
      "epoch 46; iter: 200; batch classifier loss: 0.238990; batch adversarial loss: 0.274365\n",
      "epoch 47; iter: 0; batch classifier loss: 0.294704; batch adversarial loss: 0.274017\n",
      "epoch 47; iter: 200; batch classifier loss: 0.317002; batch adversarial loss: 0.293200\n",
      "epoch 48; iter: 0; batch classifier loss: 0.251082; batch adversarial loss: 0.350049\n",
      "epoch 48; iter: 200; batch classifier loss: 0.296044; batch adversarial loss: 0.330896\n",
      "epoch 49; iter: 0; batch classifier loss: 0.252729; batch adversarial loss: 0.312475\n",
      "epoch 49; iter: 200; batch classifier loss: 0.241103; batch adversarial loss: 0.255175\n",
      "Entrenando modelo debiasado para sex...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.662513; batch adversarial loss: 0.683434\n",
      "epoch 0; iter: 200; batch classifier loss: 0.385337; batch adversarial loss: 0.657199\n",
      "epoch 1; iter: 0; batch classifier loss: 0.457940; batch adversarial loss: 0.657853\n",
      "epoch 1; iter: 200; batch classifier loss: 0.335133; batch adversarial loss: 0.633398\n",
      "epoch 2; iter: 0; batch classifier loss: 0.423433; batch adversarial loss: 0.638487\n",
      "epoch 2; iter: 200; batch classifier loss: 0.405214; batch adversarial loss: 0.605976\n",
      "epoch 3; iter: 0; batch classifier loss: 0.301894; batch adversarial loss: 0.638931\n",
      "epoch 3; iter: 200; batch classifier loss: 0.299233; batch adversarial loss: 0.639403\n",
      "epoch 4; iter: 0; batch classifier loss: 0.380253; batch adversarial loss: 0.632048\n",
      "epoch 4; iter: 200; batch classifier loss: 0.472549; batch adversarial loss: 0.614941\n",
      "epoch 5; iter: 0; batch classifier loss: 0.329149; batch adversarial loss: 0.582312\n",
      "epoch 5; iter: 200; batch classifier loss: 0.423413; batch adversarial loss: 0.682899\n",
      "epoch 6; iter: 0; batch classifier loss: 0.353037; batch adversarial loss: 0.636630\n",
      "epoch 6; iter: 200; batch classifier loss: 0.368652; batch adversarial loss: 0.652410\n",
      "epoch 7; iter: 0; batch classifier loss: 0.280969; batch adversarial loss: 0.638970\n",
      "epoch 7; iter: 200; batch classifier loss: 0.390632; batch adversarial loss: 0.605404\n",
      "epoch 8; iter: 0; batch classifier loss: 0.363204; batch adversarial loss: 0.628422\n",
      "epoch 8; iter: 200; batch classifier loss: 0.430014; batch adversarial loss: 0.563872\n",
      "epoch 9; iter: 0; batch classifier loss: 0.312595; batch adversarial loss: 0.617172\n",
      "epoch 9; iter: 200; batch classifier loss: 0.307024; batch adversarial loss: 0.590891\n",
      "epoch 10; iter: 0; batch classifier loss: 0.329823; batch adversarial loss: 0.649953\n",
      "epoch 10; iter: 200; batch classifier loss: 0.264705; batch adversarial loss: 0.694389\n",
      "epoch 11; iter: 0; batch classifier loss: 0.295345; batch adversarial loss: 0.626646\n",
      "epoch 11; iter: 200; batch classifier loss: 0.349434; batch adversarial loss: 0.562047\n",
      "epoch 12; iter: 0; batch classifier loss: 0.335238; batch adversarial loss: 0.642720\n",
      "epoch 12; iter: 200; batch classifier loss: 0.368498; batch adversarial loss: 0.613327\n",
      "epoch 13; iter: 0; batch classifier loss: 0.309671; batch adversarial loss: 0.639308\n",
      "epoch 13; iter: 200; batch classifier loss: 0.347951; batch adversarial loss: 0.605147\n",
      "epoch 14; iter: 0; batch classifier loss: 0.319997; batch adversarial loss: 0.590708\n",
      "epoch 14; iter: 200; batch classifier loss: 0.264358; batch adversarial loss: 0.646407\n",
      "epoch 15; iter: 0; batch classifier loss: 0.370247; batch adversarial loss: 0.605522\n",
      "epoch 15; iter: 200; batch classifier loss: 0.267269; batch adversarial loss: 0.599535\n",
      "epoch 16; iter: 0; batch classifier loss: 0.384118; batch adversarial loss: 0.608092\n",
      "epoch 16; iter: 200; batch classifier loss: 0.269677; batch adversarial loss: 0.656161\n",
      "epoch 17; iter: 0; batch classifier loss: 0.343635; batch adversarial loss: 0.666571\n",
      "epoch 17; iter: 200; batch classifier loss: 0.321705; batch adversarial loss: 0.658759\n",
      "epoch 18; iter: 0; batch classifier loss: 0.330626; batch adversarial loss: 0.607714\n",
      "epoch 18; iter: 200; batch classifier loss: 0.345586; batch adversarial loss: 0.590438\n",
      "epoch 19; iter: 0; batch classifier loss: 0.248305; batch adversarial loss: 0.591200\n",
      "epoch 19; iter: 200; batch classifier loss: 0.392316; batch adversarial loss: 0.615382\n",
      "epoch 20; iter: 0; batch classifier loss: 0.255213; batch adversarial loss: 0.690472\n",
      "epoch 20; iter: 200; batch classifier loss: 0.355576; batch adversarial loss: 0.598625\n",
      "epoch 21; iter: 0; batch classifier loss: 0.301335; batch adversarial loss: 0.682726\n",
      "epoch 21; iter: 200; batch classifier loss: 0.495131; batch adversarial loss: 0.567891\n",
      "epoch 22; iter: 0; batch classifier loss: 0.356221; batch adversarial loss: 0.656031\n",
      "epoch 22; iter: 200; batch classifier loss: 0.416858; batch adversarial loss: 0.658572\n",
      "epoch 23; iter: 0; batch classifier loss: 0.381588; batch adversarial loss: 0.615032\n",
      "epoch 23; iter: 200; batch classifier loss: 0.317050; batch adversarial loss: 0.648610\n",
      "epoch 24; iter: 0; batch classifier loss: 0.338984; batch adversarial loss: 0.632178\n",
      "epoch 24; iter: 200; batch classifier loss: 0.350996; batch adversarial loss: 0.595914\n",
      "epoch 25; iter: 0; batch classifier loss: 0.345573; batch adversarial loss: 0.620047\n",
      "epoch 25; iter: 200; batch classifier loss: 0.289212; batch adversarial loss: 0.658887\n",
      "epoch 26; iter: 0; batch classifier loss: 0.374393; batch adversarial loss: 0.561412\n",
      "epoch 26; iter: 200; batch classifier loss: 0.293915; batch adversarial loss: 0.636830\n",
      "epoch 27; iter: 0; batch classifier loss: 0.376036; batch adversarial loss: 0.603799\n",
      "epoch 27; iter: 200; batch classifier loss: 0.392096; batch adversarial loss: 0.596398\n",
      "epoch 28; iter: 0; batch classifier loss: 0.268767; batch adversarial loss: 0.606622\n",
      "epoch 28; iter: 200; batch classifier loss: 0.272151; batch adversarial loss: 0.614704\n",
      "epoch 29; iter: 0; batch classifier loss: 0.460579; batch adversarial loss: 0.572165\n",
      "epoch 29; iter: 200; batch classifier loss: 0.419068; batch adversarial loss: 0.569259\n",
      "epoch 30; iter: 0; batch classifier loss: 0.344800; batch adversarial loss: 0.635691\n",
      "epoch 30; iter: 200; batch classifier loss: 0.322180; batch adversarial loss: 0.700656\n",
      "epoch 31; iter: 0; batch classifier loss: 0.343621; batch adversarial loss: 0.631002\n",
      "epoch 31; iter: 200; batch classifier loss: 0.408431; batch adversarial loss: 0.605252\n",
      "epoch 32; iter: 0; batch classifier loss: 0.233670; batch adversarial loss: 0.664040\n",
      "epoch 32; iter: 200; batch classifier loss: 0.328631; batch adversarial loss: 0.575920\n",
      "epoch 33; iter: 0; batch classifier loss: 0.269802; batch adversarial loss: 0.639398\n",
      "epoch 33; iter: 200; batch classifier loss: 0.293305; batch adversarial loss: 0.644192\n",
      "epoch 34; iter: 0; batch classifier loss: 0.282203; batch adversarial loss: 0.618467\n",
      "epoch 34; iter: 200; batch classifier loss: 0.255534; batch adversarial loss: 0.679327\n",
      "epoch 35; iter: 0; batch classifier loss: 0.301930; batch adversarial loss: 0.608274\n",
      "epoch 35; iter: 200; batch classifier loss: 0.296443; batch adversarial loss: 0.626822\n",
      "epoch 36; iter: 0; batch classifier loss: 0.288089; batch adversarial loss: 0.621190\n",
      "epoch 36; iter: 200; batch classifier loss: 0.438592; batch adversarial loss: 0.614537\n",
      "epoch 37; iter: 0; batch classifier loss: 0.501746; batch adversarial loss: 0.563214\n",
      "epoch 37; iter: 200; batch classifier loss: 0.319679; batch adversarial loss: 0.579636\n",
      "epoch 38; iter: 0; batch classifier loss: 0.373642; batch adversarial loss: 0.617138\n",
      "epoch 38; iter: 200; batch classifier loss: 0.314328; batch adversarial loss: 0.638023\n",
      "epoch 39; iter: 0; batch classifier loss: 0.278546; batch adversarial loss: 0.606314\n",
      "epoch 39; iter: 200; batch classifier loss: 0.306831; batch adversarial loss: 0.650246\n",
      "epoch 40; iter: 0; batch classifier loss: 0.336520; batch adversarial loss: 0.658069\n",
      "epoch 40; iter: 200; batch classifier loss: 0.339976; batch adversarial loss: 0.612638\n",
      "epoch 41; iter: 0; batch classifier loss: 0.383392; batch adversarial loss: 0.614135\n",
      "epoch 41; iter: 200; batch classifier loss: 0.303717; batch adversarial loss: 0.671608\n",
      "epoch 42; iter: 0; batch classifier loss: 0.297104; batch adversarial loss: 0.621507\n",
      "epoch 42; iter: 200; batch classifier loss: 0.318141; batch adversarial loss: 0.707795\n",
      "epoch 43; iter: 0; batch classifier loss: 0.265664; batch adversarial loss: 0.614006\n",
      "epoch 43; iter: 200; batch classifier loss: 0.307303; batch adversarial loss: 0.636382\n",
      "epoch 44; iter: 0; batch classifier loss: 0.295352; batch adversarial loss: 0.604046\n",
      "epoch 44; iter: 200; batch classifier loss: 0.296556; batch adversarial loss: 0.632817\n",
      "epoch 45; iter: 0; batch classifier loss: 0.380473; batch adversarial loss: 0.653555\n",
      "epoch 45; iter: 200; batch classifier loss: 0.346703; batch adversarial loss: 0.691724\n",
      "epoch 46; iter: 0; batch classifier loss: 0.224850; batch adversarial loss: 0.660502\n",
      "epoch 46; iter: 200; batch classifier loss: 0.310366; batch adversarial loss: 0.647653\n",
      "epoch 47; iter: 0; batch classifier loss: 0.311618; batch adversarial loss: 0.592352\n",
      "epoch 47; iter: 200; batch classifier loss: 0.331117; batch adversarial loss: 0.589480\n",
      "epoch 48; iter: 0; batch classifier loss: 0.291181; batch adversarial loss: 0.619847\n",
      "epoch 48; iter: 200; batch classifier loss: 0.262171; batch adversarial loss: 0.598270\n",
      "epoch 49; iter: 0; batch classifier loss: 0.346161; batch adversarial loss: 0.663468\n",
      "epoch 49; iter: 200; batch classifier loss: 0.355147; batch adversarial loss: 0.654827\n"
     ]
    }
   ],
   "source": [
    "## Código que combine las dos técnicas\n",
    "predicted_test_inprocessing_2 = adversarialDebiasingProcessing(\n",
    "    train_aif_df=df_train_aif,\n",
    "    test_aif_df=df_test_aif,\n",
    "    sensitive_features=['age', 'sex'],\n",
    "    name_run_identifier='test_3'\n",
    ")\n",
    "\n",
    "post_processed_preds_2 = eqOddsPredictionProccesing(\n",
    "    test_aif_df=df_test_aif,\n",
    "    test_pred_aif_df=predicted_test_inprocessing_2,\n",
    "    sensitive_features=['age', 'sex'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr2hhtAJ1HME"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4NLywt1Z1Oju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.1415396764504229\n",
      "Disparate Impact for sex: 0.5567263263528193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<aif360.metrics.classification_metric.ClassificationMetric at 0x1655d1a5be0>,\n",
       " <aif360.metrics.classification_metric.ClassificationMetric at 0x1655f09fbf0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Código de Independencia\n",
    "get_demographic_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyfS8ENZ1LEU"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "l2aTbcPN1Qsq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Older Adults\n",
      "  True Positive Rate (TPR): 0.43\n",
      "  False Positive Rate (FPR): 0.05\n",
      "Age Group: Young Adults\n",
      "  True Positive Rate (TPR): 0.41\n",
      "  False Positive Rate (FPR): 0.05\n",
      "Sex Group: Female\n",
      "  True Positive Rate (TPR): 0.39\n",
      "  False Positive Rate (FPR): 0.05\n",
      "Sex Group: Male\n",
      "  True Positive Rate (TPR): 0.41\n",
      "  False Positive Rate (FPR): 0.05\n"
     ]
    }
   ],
   "source": [
    "# Código de Separación\n",
    "get_equalized_odds_metrics(\n",
    "    X_test_df,\n",
    "    y_test,\n",
    "    post_processed_preds_2.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du0nPRCR1CnU"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Cje5Xbu91CC5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged PPV age: 0.7356651376146789\n",
      "Unprivileged PPV age: 0.7588235294117647\n",
      "Predictive Parity Difference age: -0.023158391797085787\n",
      "Privileged PPV sex: 0.7310683585755219\n",
      "Unprivileged PPV sex: 0.7130177514792899\n",
      "Predictive Parity Difference sex: 0.018050607096231963\n"
     ]
    }
   ],
   "source": [
    "# Código de Suficiencia\n",
    "get_predictive_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8213760761531462"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, post_processed_preds_2.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDyfGMxM1ayd"
   },
   "source": [
    "## Combinación 3: Pre-procesamiento + Post-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "C9O9-ukl1aye"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=5000, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Código que combine las dos técnicas\n",
    "reweighted_train = reweighingPreprocessing(\n",
    "    train_aif_df=df_train_aif,\n",
    "    sensitive_features=['age', 'sex'],\n",
    ")\n",
    "\n",
    "logistic_model_pre = LogisticRegression(max_iter=5000, random_state=42)\n",
    "logistic_model_pre.fit(\n",
    "    reweighted_train.features, \n",
    "    reweighted_train.labels.ravel(), \n",
    "    sample_weight=reweighted_train.instance_weights\n",
    ")\n",
    "print(logistic_model_pre)\n",
    "y_pred_logistic_pre = logistic_model_pre.predict(X_test_df)\n",
    "\n",
    "\n",
    "\n",
    "post_processed_preds_3 = eqOddsPredictionProccesing(\n",
    "    df_test_aif,\n",
    "    BinaryLabelDataset(\n",
    "        df=pd.concat([X_test_df, pd.DataFrame(y_pred_logistic_pre, columns=['income'])], axis=1),\n",
    "        label_names=['income'],\n",
    "        protected_attribute_names=['age', 'sex'],\n",
    "    ),\n",
    "    ['age', 'sex'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gld5EHJy1aye"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "il05ry-y1ayf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.0934686605777317\n",
      "Disparate Impact for sex: 0.5934316437756215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<aif360.metrics.classification_metric.ClassificationMetric at 0x1655ef02540>,\n",
       " <aif360.metrics.classification_metric.ClassificationMetric at 0x1655bef9c10>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Código de Independencia\n",
    "get_demographic_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds_3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2O_Oc511ayf"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "T8gtxWls1ayf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Older Adults\n",
      "  True Positive Rate (TPR): 0.53\n",
      "  False Positive Rate (FPR): 0.08\n",
      "Age Group: Young Adults\n",
      "  True Positive Rate (TPR): 0.52\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Sex Group: Female\n",
      "  True Positive Rate (TPR): 0.51\n",
      "  False Positive Rate (FPR): 0.08\n",
      "Sex Group: Male\n",
      "  True Positive Rate (TPR): 0.53\n",
      "  False Positive Rate (FPR): 0.07\n"
     ]
    }
   ],
   "source": [
    "# Código de Separación\n",
    "get_equalized_odds_metrics(\n",
    "    X_test_df,\n",
    "    y_test,\n",
    "    post_processed_preds_3.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USapjYuh1ayf"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "E6e5O5-a1ayf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged PPV age: 0.6902876198416007\n",
      "Unprivileged PPV age: 0.7142857142857143\n",
      "Predictive Parity Difference age: -0.023998094444113605\n",
      "Privileged PPV sex: 0.7310683585755219\n",
      "Unprivileged PPV sex: 0.7130177514792899\n",
      "Predictive Parity Difference sex: 0.018050607096231963\n"
     ]
    }
   ],
   "source": [
    "# Código de Suficiencia\n",
    "get_predictive_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds_3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8283054525092741"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, post_processed_preds_3.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmNHfI0d1tyZ"
   },
   "source": [
    "## Combinación 4: Pre-procesamiento + In-Procesamiento + Post-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_L3bpAGk1tyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo debiasado para age...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.777549; batch adversarial loss: 1.184572\n",
      "epoch 0; iter: 200; batch classifier loss: 1.427868; batch adversarial loss: 1.104806\n",
      "epoch 1; iter: 0; batch classifier loss: 1.680545; batch adversarial loss: 1.040749\n",
      "epoch 1; iter: 200; batch classifier loss: 1.638614; batch adversarial loss: 0.769586\n",
      "epoch 2; iter: 0; batch classifier loss: 1.473621; batch adversarial loss: 0.686931\n",
      "epoch 2; iter: 200; batch classifier loss: 0.839579; batch adversarial loss: 0.544076\n",
      "epoch 3; iter: 0; batch classifier loss: 0.648701; batch adversarial loss: 0.494866\n",
      "epoch 3; iter: 200; batch classifier loss: 0.424854; batch adversarial loss: 0.452438\n",
      "epoch 4; iter: 0; batch classifier loss: 0.321262; batch adversarial loss: 0.442021\n",
      "epoch 4; iter: 200; batch classifier loss: 0.282002; batch adversarial loss: 0.432926\n",
      "epoch 5; iter: 0; batch classifier loss: 0.398796; batch adversarial loss: 0.389925\n",
      "epoch 5; iter: 200; batch classifier loss: 0.303011; batch adversarial loss: 0.359397\n",
      "epoch 6; iter: 0; batch classifier loss: 0.359660; batch adversarial loss: 0.329974\n",
      "epoch 6; iter: 200; batch classifier loss: 0.344681; batch adversarial loss: 0.303009\n",
      "epoch 7; iter: 0; batch classifier loss: 0.275723; batch adversarial loss: 0.419352\n",
      "epoch 7; iter: 200; batch classifier loss: 0.444441; batch adversarial loss: 0.307136\n",
      "epoch 8; iter: 0; batch classifier loss: 0.381094; batch adversarial loss: 0.362876\n",
      "epoch 8; iter: 200; batch classifier loss: 0.300951; batch adversarial loss: 0.350264\n",
      "epoch 9; iter: 0; batch classifier loss: 0.375021; batch adversarial loss: 0.291117\n",
      "epoch 9; iter: 200; batch classifier loss: 0.289093; batch adversarial loss: 0.222208\n",
      "epoch 10; iter: 0; batch classifier loss: 0.234452; batch adversarial loss: 0.388696\n",
      "epoch 10; iter: 200; batch classifier loss: 0.303741; batch adversarial loss: 0.324073\n",
      "epoch 11; iter: 0; batch classifier loss: 0.277248; batch adversarial loss: 0.277164\n",
      "epoch 11; iter: 200; batch classifier loss: 0.335743; batch adversarial loss: 0.263023\n",
      "epoch 12; iter: 0; batch classifier loss: 0.353508; batch adversarial loss: 0.251704\n",
      "epoch 12; iter: 200; batch classifier loss: 0.367086; batch adversarial loss: 0.301489\n",
      "epoch 13; iter: 0; batch classifier loss: 0.287896; batch adversarial loss: 0.279823\n",
      "epoch 13; iter: 200; batch classifier loss: 0.238696; batch adversarial loss: 0.312959\n",
      "epoch 14; iter: 0; batch classifier loss: 0.333349; batch adversarial loss: 0.356567\n",
      "epoch 14; iter: 200; batch classifier loss: 0.283618; batch adversarial loss: 0.234107\n",
      "epoch 15; iter: 0; batch classifier loss: 0.308660; batch adversarial loss: 0.364132\n",
      "epoch 15; iter: 200; batch classifier loss: 0.331183; batch adversarial loss: 0.316766\n",
      "epoch 16; iter: 0; batch classifier loss: 0.265796; batch adversarial loss: 0.227486\n",
      "epoch 16; iter: 200; batch classifier loss: 0.371242; batch adversarial loss: 0.285381\n",
      "epoch 17; iter: 0; batch classifier loss: 0.278796; batch adversarial loss: 0.307976\n",
      "epoch 17; iter: 200; batch classifier loss: 0.339587; batch adversarial loss: 0.247130\n",
      "epoch 18; iter: 0; batch classifier loss: 0.354115; batch adversarial loss: 0.258565\n",
      "epoch 18; iter: 200; batch classifier loss: 0.304618; batch adversarial loss: 0.242148\n",
      "epoch 19; iter: 0; batch classifier loss: 0.316664; batch adversarial loss: 0.306419\n",
      "epoch 19; iter: 200; batch classifier loss: 0.243889; batch adversarial loss: 0.201287\n",
      "epoch 20; iter: 0; batch classifier loss: 0.328909; batch adversarial loss: 0.288845\n",
      "epoch 20; iter: 200; batch classifier loss: 0.278843; batch adversarial loss: 0.260796\n",
      "epoch 21; iter: 0; batch classifier loss: 0.270332; batch adversarial loss: 0.217340\n",
      "epoch 21; iter: 200; batch classifier loss: 0.271789; batch adversarial loss: 0.233588\n",
      "epoch 22; iter: 0; batch classifier loss: 0.311197; batch adversarial loss: 0.161448\n",
      "epoch 22; iter: 200; batch classifier loss: 0.372804; batch adversarial loss: 0.296356\n",
      "epoch 23; iter: 0; batch classifier loss: 0.362859; batch adversarial loss: 0.269343\n",
      "epoch 23; iter: 200; batch classifier loss: 0.329006; batch adversarial loss: 0.349714\n",
      "epoch 24; iter: 0; batch classifier loss: 0.284466; batch adversarial loss: 0.181669\n",
      "epoch 24; iter: 200; batch classifier loss: 0.265372; batch adversarial loss: 0.293800\n",
      "epoch 25; iter: 0; batch classifier loss: 0.264074; batch adversarial loss: 0.261236\n",
      "epoch 25; iter: 200; batch classifier loss: 0.280793; batch adversarial loss: 0.347528\n",
      "epoch 26; iter: 0; batch classifier loss: 0.203382; batch adversarial loss: 0.332477\n",
      "epoch 26; iter: 200; batch classifier loss: 0.312630; batch adversarial loss: 0.237900\n",
      "epoch 27; iter: 0; batch classifier loss: 0.234036; batch adversarial loss: 0.255364\n",
      "epoch 27; iter: 200; batch classifier loss: 0.351487; batch adversarial loss: 0.255789\n",
      "epoch 28; iter: 0; batch classifier loss: 0.299860; batch adversarial loss: 0.255455\n",
      "epoch 28; iter: 200; batch classifier loss: 0.312336; batch adversarial loss: 0.292692\n",
      "epoch 29; iter: 0; batch classifier loss: 0.207179; batch adversarial loss: 0.310052\n",
      "epoch 29; iter: 200; batch classifier loss: 0.269483; batch adversarial loss: 0.200343\n",
      "epoch 30; iter: 0; batch classifier loss: 0.226167; batch adversarial loss: 0.366314\n",
      "epoch 30; iter: 200; batch classifier loss: 0.292155; batch adversarial loss: 0.330422\n",
      "epoch 31; iter: 0; batch classifier loss: 0.292126; batch adversarial loss: 0.291954\n",
      "epoch 31; iter: 200; batch classifier loss: 0.268709; batch adversarial loss: 0.275574\n",
      "epoch 32; iter: 0; batch classifier loss: 0.358983; batch adversarial loss: 0.197801\n",
      "epoch 32; iter: 200; batch classifier loss: 0.307056; batch adversarial loss: 0.329636\n",
      "epoch 33; iter: 0; batch classifier loss: 0.305890; batch adversarial loss: 0.218419\n",
      "epoch 33; iter: 200; batch classifier loss: 0.297921; batch adversarial loss: 0.313778\n",
      "epoch 34; iter: 0; batch classifier loss: 0.233190; batch adversarial loss: 0.217417\n",
      "epoch 34; iter: 200; batch classifier loss: 0.287005; batch adversarial loss: 0.312408\n",
      "epoch 35; iter: 0; batch classifier loss: 0.267673; batch adversarial loss: 0.256902\n",
      "epoch 35; iter: 200; batch classifier loss: 0.296180; batch adversarial loss: 0.328829\n",
      "epoch 36; iter: 0; batch classifier loss: 0.228387; batch adversarial loss: 0.272368\n",
      "epoch 36; iter: 200; batch classifier loss: 0.230350; batch adversarial loss: 0.351930\n",
      "epoch 37; iter: 0; batch classifier loss: 0.341343; batch adversarial loss: 0.217170\n",
      "epoch 37; iter: 200; batch classifier loss: 0.202405; batch adversarial loss: 0.331795\n",
      "epoch 38; iter: 0; batch classifier loss: 0.285530; batch adversarial loss: 0.310772\n",
      "epoch 38; iter: 200; batch classifier loss: 0.222880; batch adversarial loss: 0.217127\n",
      "epoch 39; iter: 0; batch classifier loss: 0.300987; batch adversarial loss: 0.314335\n",
      "epoch 39; iter: 200; batch classifier loss: 0.302636; batch adversarial loss: 0.275175\n",
      "epoch 40; iter: 0; batch classifier loss: 0.239796; batch adversarial loss: 0.236121\n",
      "epoch 40; iter: 200; batch classifier loss: 0.316222; batch adversarial loss: 0.254457\n",
      "epoch 41; iter: 0; batch classifier loss: 0.302845; batch adversarial loss: 0.294453\n",
      "epoch 41; iter: 200; batch classifier loss: 0.315084; batch adversarial loss: 0.329150\n",
      "epoch 42; iter: 0; batch classifier loss: 0.324615; batch adversarial loss: 0.293269\n",
      "epoch 42; iter: 200; batch classifier loss: 0.259634; batch adversarial loss: 0.297083\n",
      "epoch 43; iter: 0; batch classifier loss: 0.294646; batch adversarial loss: 0.236279\n",
      "epoch 43; iter: 200; batch classifier loss: 0.304157; batch adversarial loss: 0.217455\n",
      "epoch 44; iter: 0; batch classifier loss: 0.276326; batch adversarial loss: 0.311017\n",
      "epoch 44; iter: 200; batch classifier loss: 0.343559; batch adversarial loss: 0.296446\n",
      "epoch 45; iter: 0; batch classifier loss: 0.304292; batch adversarial loss: 0.292156\n",
      "epoch 45; iter: 200; batch classifier loss: 0.289973; batch adversarial loss: 0.311408\n",
      "epoch 46; iter: 0; batch classifier loss: 0.232280; batch adversarial loss: 0.421865\n",
      "epoch 46; iter: 200; batch classifier loss: 0.292477; batch adversarial loss: 0.291680\n",
      "epoch 47; iter: 0; batch classifier loss: 0.218929; batch adversarial loss: 0.317458\n",
      "epoch 47; iter: 200; batch classifier loss: 0.225174; batch adversarial loss: 0.327302\n",
      "epoch 48; iter: 0; batch classifier loss: 0.244825; batch adversarial loss: 0.272844\n",
      "epoch 48; iter: 200; batch classifier loss: 0.329132; batch adversarial loss: 0.253407\n",
      "epoch 49; iter: 0; batch classifier loss: 0.276153; batch adversarial loss: 0.310210\n",
      "epoch 49; iter: 200; batch classifier loss: 0.230808; batch adversarial loss: 0.234930\n",
      "Entrenando modelo debiasado para sex...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.746574; batch adversarial loss: 0.644648\n",
      "epoch 0; iter: 200; batch classifier loss: 0.463273; batch adversarial loss: 0.640023\n",
      "epoch 1; iter: 0; batch classifier loss: 0.415381; batch adversarial loss: 0.672871\n",
      "epoch 1; iter: 200; batch classifier loss: 0.443195; batch adversarial loss: 0.673408\n",
      "epoch 2; iter: 0; batch classifier loss: 0.486272; batch adversarial loss: 0.660199\n",
      "epoch 2; iter: 200; batch classifier loss: 0.425423; batch adversarial loss: 0.663112\n",
      "epoch 3; iter: 0; batch classifier loss: 0.439288; batch adversarial loss: 0.617648\n",
      "epoch 3; iter: 200; batch classifier loss: 0.320182; batch adversarial loss: 0.642116\n",
      "epoch 4; iter: 0; batch classifier loss: 0.429976; batch adversarial loss: 0.624289\n",
      "epoch 4; iter: 200; batch classifier loss: 0.431344; batch adversarial loss: 0.635808\n",
      "epoch 5; iter: 0; batch classifier loss: 0.337417; batch adversarial loss: 0.668831\n",
      "epoch 5; iter: 200; batch classifier loss: 0.347060; batch adversarial loss: 0.644246\n",
      "epoch 6; iter: 0; batch classifier loss: 0.315548; batch adversarial loss: 0.667623\n",
      "epoch 6; iter: 200; batch classifier loss: 0.315478; batch adversarial loss: 0.625921\n",
      "epoch 7; iter: 0; batch classifier loss: 0.399110; batch adversarial loss: 0.593774\n",
      "epoch 7; iter: 200; batch classifier loss: 0.311703; batch adversarial loss: 0.580316\n",
      "epoch 8; iter: 0; batch classifier loss: 0.277175; batch adversarial loss: 0.640803\n",
      "epoch 8; iter: 200; batch classifier loss: 0.394583; batch adversarial loss: 0.616671\n",
      "epoch 9; iter: 0; batch classifier loss: 0.341212; batch adversarial loss: 0.636211\n",
      "epoch 9; iter: 200; batch classifier loss: 0.304742; batch adversarial loss: 0.625780\n",
      "epoch 10; iter: 0; batch classifier loss: 0.321341; batch adversarial loss: 0.633992\n",
      "epoch 10; iter: 200; batch classifier loss: 0.283926; batch adversarial loss: 0.669072\n",
      "epoch 11; iter: 0; batch classifier loss: 0.344825; batch adversarial loss: 0.648302\n",
      "epoch 11; iter: 200; batch classifier loss: 0.361102; batch adversarial loss: 0.643060\n",
      "epoch 12; iter: 0; batch classifier loss: 0.288280; batch adversarial loss: 0.663640\n",
      "epoch 12; iter: 200; batch classifier loss: 0.389989; batch adversarial loss: 0.614284\n",
      "epoch 13; iter: 0; batch classifier loss: 0.392738; batch adversarial loss: 0.615272\n",
      "epoch 13; iter: 200; batch classifier loss: 0.341116; batch adversarial loss: 0.643383\n",
      "epoch 14; iter: 0; batch classifier loss: 0.334411; batch adversarial loss: 0.639104\n",
      "epoch 14; iter: 200; batch classifier loss: 0.308226; batch adversarial loss: 0.661891\n",
      "epoch 15; iter: 0; batch classifier loss: 0.302506; batch adversarial loss: 0.583649\n",
      "epoch 15; iter: 200; batch classifier loss: 0.300172; batch adversarial loss: 0.622326\n",
      "epoch 16; iter: 0; batch classifier loss: 0.324557; batch adversarial loss: 0.594327\n",
      "epoch 16; iter: 200; batch classifier loss: 0.328810; batch adversarial loss: 0.609430\n",
      "epoch 17; iter: 0; batch classifier loss: 0.324847; batch adversarial loss: 0.528677\n",
      "epoch 17; iter: 200; batch classifier loss: 0.329794; batch adversarial loss: 0.574001\n",
      "epoch 18; iter: 0; batch classifier loss: 0.375043; batch adversarial loss: 0.617503\n",
      "epoch 18; iter: 200; batch classifier loss: 0.299456; batch adversarial loss: 0.607146\n",
      "epoch 19; iter: 0; batch classifier loss: 0.295978; batch adversarial loss: 0.657498\n",
      "epoch 19; iter: 200; batch classifier loss: 0.278634; batch adversarial loss: 0.604297\n",
      "epoch 20; iter: 0; batch classifier loss: 0.354395; batch adversarial loss: 0.666095\n",
      "epoch 20; iter: 200; batch classifier loss: 0.269822; batch adversarial loss: 0.684984\n",
      "epoch 21; iter: 0; batch classifier loss: 0.321157; batch adversarial loss: 0.604437\n",
      "epoch 21; iter: 200; batch classifier loss: 0.383588; batch adversarial loss: 0.631219\n",
      "epoch 22; iter: 0; batch classifier loss: 0.285794; batch adversarial loss: 0.623290\n",
      "epoch 22; iter: 200; batch classifier loss: 0.382006; batch adversarial loss: 0.667008\n",
      "epoch 23; iter: 0; batch classifier loss: 0.344708; batch adversarial loss: 0.608487\n",
      "epoch 23; iter: 200; batch classifier loss: 0.330743; batch adversarial loss: 0.592158\n",
      "epoch 24; iter: 0; batch classifier loss: 0.345457; batch adversarial loss: 0.631565\n",
      "epoch 24; iter: 200; batch classifier loss: 0.269950; batch adversarial loss: 0.586007\n",
      "epoch 25; iter: 0; batch classifier loss: 0.299475; batch adversarial loss: 0.629176\n",
      "epoch 25; iter: 200; batch classifier loss: 0.354319; batch adversarial loss: 0.593858\n",
      "epoch 26; iter: 0; batch classifier loss: 0.301268; batch adversarial loss: 0.663743\n",
      "epoch 26; iter: 200; batch classifier loss: 0.356521; batch adversarial loss: 0.641861\n",
      "epoch 27; iter: 0; batch classifier loss: 0.297485; batch adversarial loss: 0.653833\n",
      "epoch 27; iter: 200; batch classifier loss: 0.354105; batch adversarial loss: 0.577507\n",
      "epoch 28; iter: 0; batch classifier loss: 0.327629; batch adversarial loss: 0.708637\n",
      "epoch 28; iter: 200; batch classifier loss: 0.346038; batch adversarial loss: 0.574114\n",
      "epoch 29; iter: 0; batch classifier loss: 0.295728; batch adversarial loss: 0.614026\n",
      "epoch 29; iter: 200; batch classifier loss: 0.370343; batch adversarial loss: 0.619383\n",
      "epoch 30; iter: 0; batch classifier loss: 0.219977; batch adversarial loss: 0.701282\n",
      "epoch 30; iter: 200; batch classifier loss: 0.359622; batch adversarial loss: 0.607612\n",
      "epoch 31; iter: 0; batch classifier loss: 0.316368; batch adversarial loss: 0.608933\n",
      "epoch 31; iter: 200; batch classifier loss: 0.263328; batch adversarial loss: 0.682143\n",
      "epoch 32; iter: 0; batch classifier loss: 0.328639; batch adversarial loss: 0.613486\n",
      "epoch 32; iter: 200; batch classifier loss: 0.264653; batch adversarial loss: 0.579348\n",
      "epoch 33; iter: 0; batch classifier loss: 0.332851; batch adversarial loss: 0.646525\n",
      "epoch 33; iter: 200; batch classifier loss: 0.418479; batch adversarial loss: 0.575119\n",
      "epoch 34; iter: 0; batch classifier loss: 0.372970; batch adversarial loss: 0.621265\n",
      "epoch 34; iter: 200; batch classifier loss: 0.367972; batch adversarial loss: 0.597423\n",
      "epoch 35; iter: 0; batch classifier loss: 0.340296; batch adversarial loss: 0.540055\n",
      "epoch 35; iter: 200; batch classifier loss: 0.397950; batch adversarial loss: 0.582164\n",
      "epoch 36; iter: 0; batch classifier loss: 0.323154; batch adversarial loss: 0.617557\n",
      "epoch 36; iter: 200; batch classifier loss: 0.327365; batch adversarial loss: 0.632585\n",
      "epoch 37; iter: 0; batch classifier loss: 0.298864; batch adversarial loss: 0.669234\n",
      "epoch 37; iter: 200; batch classifier loss: 0.345122; batch adversarial loss: 0.649294\n",
      "epoch 38; iter: 0; batch classifier loss: 0.270158; batch adversarial loss: 0.664822\n",
      "epoch 38; iter: 200; batch classifier loss: 0.375527; batch adversarial loss: 0.641025\n",
      "epoch 39; iter: 0; batch classifier loss: 0.287391; batch adversarial loss: 0.610034\n",
      "epoch 39; iter: 200; batch classifier loss: 0.418737; batch adversarial loss: 0.603173\n",
      "epoch 40; iter: 0; batch classifier loss: 0.319782; batch adversarial loss: 0.580425\n",
      "epoch 40; iter: 200; batch classifier loss: 0.330966; batch adversarial loss: 0.620026\n",
      "epoch 41; iter: 0; batch classifier loss: 0.216815; batch adversarial loss: 0.679006\n",
      "epoch 41; iter: 200; batch classifier loss: 0.327122; batch adversarial loss: 0.627148\n",
      "epoch 42; iter: 0; batch classifier loss: 0.258512; batch adversarial loss: 0.658001\n",
      "epoch 42; iter: 200; batch classifier loss: 0.301959; batch adversarial loss: 0.645336\n",
      "epoch 43; iter: 0; batch classifier loss: 0.301048; batch adversarial loss: 0.634776\n",
      "epoch 43; iter: 200; batch classifier loss: 0.357910; batch adversarial loss: 0.576502\n",
      "epoch 44; iter: 0; batch classifier loss: 0.342517; batch adversarial loss: 0.571439\n",
      "epoch 44; iter: 200; batch classifier loss: 0.313813; batch adversarial loss: 0.584246\n",
      "epoch 45; iter: 0; batch classifier loss: 0.298455; batch adversarial loss: 0.547443\n",
      "epoch 45; iter: 200; batch classifier loss: 0.278150; batch adversarial loss: 0.599242\n",
      "epoch 46; iter: 0; batch classifier loss: 0.314942; batch adversarial loss: 0.567751\n",
      "epoch 46; iter: 200; batch classifier loss: 0.251932; batch adversarial loss: 0.682993\n",
      "epoch 47; iter: 0; batch classifier loss: 0.301707; batch adversarial loss: 0.592333\n",
      "epoch 47; iter: 200; batch classifier loss: 0.369859; batch adversarial loss: 0.664698\n",
      "epoch 48; iter: 0; batch classifier loss: 0.320502; batch adversarial loss: 0.637677\n",
      "epoch 48; iter: 200; batch classifier loss: 0.354680; batch adversarial loss: 0.627298\n",
      "epoch 49; iter: 0; batch classifier loss: 0.287090; batch adversarial loss: 0.617833\n",
      "epoch 49; iter: 200; batch classifier loss: 0.299586; batch adversarial loss: 0.597960\n"
     ]
    }
   ],
   "source": [
    "## Código que combine las dos técnicas\n",
    "rewighted_train = reweighingPreprocessing(\n",
    "    train_aif_df=df_train_aif,\n",
    "    sensitive_features=['age', 'sex'],\n",
    ")\n",
    "\n",
    "predicted_test_inprocessing_4 = adversarialDebiasingProcessing(\n",
    "    train_aif_df=rewighted_train,\n",
    "    test_aif_df=df_test_aif,\n",
    "    sensitive_features=['age', 'sex'],\n",
    "    name_run_identifier='test_4'\n",
    ")\n",
    "\n",
    "post_processed_preds_4 = eqOddsPredictionProccesing(\n",
    "    df_test_aif,\n",
    "    predicted_test_inprocessing_4,\n",
    "    ['age', 'sex'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1x4Qg6m1tya"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UduOtFUO1tya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.0866878267676674\n",
      "Disparate Impact for sex: 0.5983967732095041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<aif360.metrics.classification_metric.ClassificationMetric at 0x1655f2dec30>,\n",
       " <aif360.metrics.classification_metric.ClassificationMetric at 0x1653aec8980>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Código de Independencia\n",
    "get_demographic_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds_4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sth3VWqq1tya"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0hPLa-Sh1tya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Older Adults\n",
      "  True Positive Rate (TPR): 0.47\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Age Group: Young Adults\n",
      "  True Positive Rate (TPR): 0.46\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Sex Group: Female\n",
      "  True Positive Rate (TPR): 0.48\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Sex Group: Male\n",
      "  True Positive Rate (TPR): 0.46\n",
      "  False Positive Rate (FPR): 0.07\n"
     ]
    }
   ],
   "source": [
    "# Código de Separación\n",
    "get_equalized_odds_metrics(\n",
    "    X_test_df,\n",
    "    y_test,\n",
    "    post_processed_preds_4.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue09jjXI1tya"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1yRr99hB1tyb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged PPV age: 0.688177107866227\n",
      "Unprivileged PPV age: 0.7208121827411168\n",
      "Predictive Parity Difference age: -0.03263507487488981\n",
      "Privileged PPV sex: 0.7310683585755219\n",
      "Unprivileged PPV sex: 0.7130177514792899\n",
      "Predictive Parity Difference sex: 0.018050607096231963\n"
     ]
    }
   ],
   "source": [
    "# Código de Suficiencia\n",
    "get_predictive_parity_metrics(\n",
    "    df_test_aif,\n",
    "    post_processed_preds_4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8196962273395394"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, post_processed_preds_4.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
