{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rLfIEI2ZI5c"
   },
   "source": [
    "## Instalar Librerías e Importar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wD980A30LRmC",
    "outputId": "b9a995fa-db56-481f-d3cc-b4c88a2bd126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'aif360[Reductions]'\"\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo aif360 'aif360[Reductions]' 'aif360[inFairness]' tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IMyZSOOsZa1X",
    "outputId": "a22ff9b8-085d-4e9a-b25a-ee3f74b38b9d"
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4cOKrsx6Lk3m",
    "outputId": "a0153edf-245a-4111-dd16-d72b3f595d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "# metadata\n",
    "print(adult.metadata)\n",
    "\n",
    "# variable information\n",
    "print(adult.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucfynq5YaWd-"
   },
   "source": [
    "## Preparar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "_3ppeC1oMN2Z",
    "outputId": "d5dbb815-f7da-40a5-edeb-72358ddb8100"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_labels(label: str):\n",
    "    return label.replace(\".\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "y_df = pd.DataFrame(y)\n",
    "\n",
    "y_df = y_df.map(clean_labels)\n",
    "\n",
    "df = pd.concat([X_df, y_df], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47621, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ssZ8SjneQN8X"
   },
   "outputs": [],
   "source": [
    "X_df_clean = df.drop(columns=['income'])\n",
    "y_df_clean = df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QS-zM6tTMqP2"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df_clean, y_df_clean, test_size=0.3, stratify=y_df_clean, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=X_df_clean.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_df_clean.columns)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns=['income'])\n",
    "y_test = pd.DataFrame(y_test, columns=['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex map\n",
    "mapping_sex = {\"Female\": 0, \"Male\": 1}\n",
    "# Income map\n",
    "mapping_income = {\"<=50K\": 0, \">50K\": 1}\n",
    "\n",
    "def map_col(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    mapping: dict,\n",
    ") -> pd.DataFrame:\n",
    "    df[col] = df[col].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing train data\n",
    "X_train['age'] = X_train['age'].apply(lambda x: 1 if x < 60 else 0)\n",
    "X_train = map_col(X_train, \"sex\", mapping_sex)\n",
    "\n",
    "y_train = map_col(y_train, \"income\", mapping_income)\n",
    "\n",
    "# Preprocessing test data\n",
    "X_test['age'] = X_test['age'].apply(lambda x: 1 if x < 60 else 0)\n",
    "X_test = map_col(X_test, \"sex\", mapping_sex)\n",
    "\n",
    "y_test = map_col(y_test, \"income\", mapping_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26818</th>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>202033</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28252</th>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>340599</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9258</th>\n",
       "      <td>1</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>177035</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41064</th>\n",
       "      <td>1</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>30012</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30461</th>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>185216</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt     education  education-num  \\\n",
       "26818    1           Private  202033     Bachelors             13   \n",
       "28252    1           Private  340599          11th              7   \n",
       "9258     1         State-gov  177035          11th              7   \n",
       "41064    1  Self-emp-not-inc   30012  Some-college             10   \n",
       "30461    1           Private  185216  Some-college             10   \n",
       "\n",
       "           marital-status      occupation relationship   race  sex  \\\n",
       "26818  Married-civ-spouse  Prof-specialty      Husband  White    1   \n",
       "28252           Separated   Other-service    Unmarried  Black    0   \n",
       "9258             Divorced   Other-service    Unmarried  White    0   \n",
       "41064  Married-civ-spouse           Sales      Husband  White    1   \n",
       "30461       Never-married    Adm-clerical    Own-child  White    1   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country  \n",
       "26818             0             0              40  United-States  \n",
       "28252             0             0              40  United-States  \n",
       "9258              0             0              40  United-States  \n",
       "41064             0             0              50  United-States  \n",
       "30461             0             0              40  United-States  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26818</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28252</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9258</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41064</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30461</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       income\n",
       "26818       0\n",
       "28252       0\n",
       "9258        0\n",
       "41064       1\n",
       "30461       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33334, 107), (14287, 107))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "standard_scaler = MinMaxScaler()\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Aplicamos directamente a los datos\n",
    "X_numeric_train_scaled = standard_scaler.fit_transform(X_train[numeric_cols])\n",
    "X_numeric_test_scaled = standard_scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Aplicamos one hot encoding\n",
    "X_categorical_train = one_hot_encoder.fit_transform(X_train[categorical_cols])\n",
    "X_categorical_test = one_hot_encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "X_train_processed = np.concatenate([X_numeric_train_scaled, X_categorical_train.toarray()], axis=1)\n",
    "X_test_processed = np.concatenate([X_numeric_test_scaled, X_categorical_test.toarray()], axis=1)\n",
    "X_train_processed.shape, X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() + one_hot_encoder.get_feature_names_out(categorical_cols).tolist())\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=numeric_cols.tolist() + one_hot_encoder.get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_?</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.127659</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221481</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110733</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116273</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age    fnlwgt  education-num  sex  capital-gain  capital-loss  \\\n",
       "0  1.0  0.127659            0.8  1.0           0.0           0.0   \n",
       "1  1.0  0.221481            0.4  0.0           0.0           0.0   \n",
       "2  1.0  0.110733            0.4  0.0           0.0           0.0   \n",
       "3  1.0  0.011186            0.6  1.0           0.0           0.0   \n",
       "4  1.0  0.116273            0.6  1.0           0.0           0.0   \n",
       "\n",
       "   hours-per-week  workclass_?  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0        0.397959          0.0                    0.0                  0.0   \n",
       "1        0.397959          0.0                    0.0                  0.0   \n",
       "2        0.397959          0.0                    0.0                  0.0   \n",
       "3        0.500000          0.0                    0.0                  0.0   \n",
       "4        0.397959          0.0                    0.0                  0.0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                      0.0                         0.0   \n",
       "1  ...                      0.0                         0.0   \n",
       "2  ...                      0.0                         0.0   \n",
       "3  ...                      0.0                         0.0   \n",
       "4  ...                      0.0                         0.0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                      0.0                   0.0                    0.0   \n",
       "1                      0.0                   0.0                    0.0   \n",
       "2                      0.0                   0.0                    0.0   \n",
       "3                      0.0                   0.0                    0.0   \n",
       "4                      0.0                   0.0                    0.0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                      0.0                             0.0   \n",
       "1                      0.0                             0.0   \n",
       "2                      0.0                             0.0   \n",
       "3                      0.0                             0.0   \n",
       "4                      0.0                             0.0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                           1.0                     0.0   \n",
       "1                           1.0                     0.0   \n",
       "2                           1.0                     0.0   \n",
       "3                           1.0                     0.0   \n",
       "4                           1.0                     0.0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                        0.0  \n",
       "1                        0.0  \n",
       "2                        0.0  \n",
       "3                        0.0  \n",
       "4                        0.0  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFD_er0Qay6x"
   },
   "source": [
    "# Entrenamiento de Modelos\n",
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfCR6vvnR_0f",
    "outputId": "c3294992-cc9f-482f-fb69-68d75c94805a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression Model: 0.85\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "logistic_regression.fit(X_train_df, y_train)\n",
    "logistic_pred = logistic_regression.predict(X_test_df)\n",
    "acc_lr = accuracy_score(y_test, logistic_pred)\n",
    "print(f'Accuracy for Logistic Regression Model: {acc_lr:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvPP0KmQbYo_"
   },
   "source": [
    "### Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcmXK0xCSO71",
    "outputId": "41e94751-b10f-4658-ff9e-9468d12062fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest Classifier Model: 0.83\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(X_train_df, y_train)\n",
    "randomF_pred = random_forest.predict(X_test_df)\n",
    "acc_rf = accuracy_score(y_test, randomF_pred)\n",
    "print(f'Accuracy for Random Forest Classifier Model: {acc_rf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KcElJCtb0sn"
   },
   "source": [
    "### K Neighbors Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQ-ljNymSvSC",
    "outputId": "c77636bf-61ea-4841-c284-4b4a8cc7c75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for K Neighbors Classifier Model: 0.82\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train_df, y_train)\n",
    "knn_pred = knn.predict(X_test_df)\n",
    "acc_kn = accuracy_score(y_test, knn_pred)\n",
    "print(f'Accuracy for K Neighbors Classifier Model: {acc_kn:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp5ENmaTcb00"
   },
   "source": [
    "# Métricas de Equidad\n",
    "#### Modelo escogido por mejor rendimiento: **Logistic Regression Model**\n",
    "#### Atributos sensibles a estudiar: **age** y **sex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukUHvkSTdIlI"
   },
   "source": [
    "## Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dud7uJs4EdZX"
   },
   "outputs": [],
   "source": [
    "best_pred = logistic_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "y1gE7xoaG2Cf"
   },
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "df_test_aif = BinaryLabelDataset(\n",
    "    df=pd.concat([X_test_df, pd.DataFrame(y_test, columns=['income'])], axis=1),\n",
    "    label_names=['income'],\n",
    "    protected_attribute_names=['age', 'sex'],\n",
    ")\n",
    "\n",
    "df_test_pred_aif = df_test_aif.copy(deepcopy=True)\n",
    "df_test_pred_aif.labels = best_pred.reshape(-1, 1)\n",
    "\n",
    "df_train_aif = BinaryLabelDataset(\n",
    "    df=pd.concat([X_train_df, pd.DataFrame(y_train, columns=['income'])], axis=1),\n",
    "    label_names=['income'],\n",
    "    protected_attribute_names=['age', 'sex'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e5yJhS1-mdM",
    "outputId": "fa00d4a0-9beb-4e1d-d23a-8fb73bd3c5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.050856421312152\n",
      "Disparate Impact for sex: 0.2816774955165621\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Para 'age'\n",
    "metric_age = ClassificationMetric(\n",
    "    df_test_aif,\n",
    "    df_test_pred_aif,\n",
    "    privileged_groups=[{\"age\": 1}],\n",
    "    unprivileged_groups=[{\"age\": 0}]\n",
    ")\n",
    "print(\"Disparate Impact for age:\", metric_age.disparate_impact())\n",
    "\n",
    "# Para 'sex'\n",
    "metric_sex = ClassificationMetric(\n",
    "    df_test_aif,\n",
    "    df_test_pred_aif,\n",
    "    privileged_groups=[{\"sex\": 1}],\n",
    "    unprivileged_groups=[{\"sex\": 0}]\n",
    ")\n",
    "print(\"Disparate Impact for sex:\", metric_sex.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcMp7nFSdUQp"
   },
   "source": [
    "## Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3Yr05DpXe-7n"
   },
   "outputs": [],
   "source": [
    "def map_col(df, col, mapping) -> pd.DataFrame:\n",
    "    df[col] = df[col].map(mapping)\n",
    "    return df\n",
    "\n",
    "def calculate_tpr_fpr_sex(data, subgroup):\n",
    "    group_data = data[data['sex'] == subgroup]\n",
    "    true_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 1))\n",
    "    false_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 0))\n",
    "    total_positive = np.sum(group_data['y_true'] == 1)\n",
    "    total_negative = np.sum(group_data['y_true'] == 0)\n",
    "\n",
    "    tpr = true_positive / total_positive if total_positive > 0 else 0\n",
    "    fpr = false_positive / total_negative if total_negative > 0 else 0\n",
    "\n",
    "    return tpr, fpr\n",
    "\n",
    "def calculate_tpr_fpr_age(data, subgroup):\n",
    "    group_data = data[data['age'] == subgroup]\n",
    "    true_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 1))\n",
    "    false_positive = np.sum((group_data['y_pred'] == 1) & (group_data['y_true'] == 0))\n",
    "    total_positive = np.sum(group_data['y_true'] == 1)\n",
    "    total_negative = np.sum(group_data['y_true'] == 0)\n",
    "\n",
    "    tpr = true_positive / total_positive if total_positive > 0 else 0\n",
    "    fpr = false_positive / total_negative if total_negative > 0 else 0\n",
    "\n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGLRTJKJgNrj"
   },
   "source": [
    "### Equalized Odds for **age** atribute\n",
    "Map: [**0: Older Adults**, **1: Young Adults**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_to_separacion(X_test_dataframe, y_test, y_pred):\n",
    "    return pd.concat([\n",
    "        X_test_dataframe,\n",
    "        pd.DataFrame(y_test, columns=['y_true']),\n",
    "        pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "    ], axis=1)\n",
    "    \n",
    "separacion_df = prepare_df_to_separacion(X_test_df, y_test, best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkQqJSfzIPTO",
    "outputId": "fef17477-567c-48b6-a567-46085afa7dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Older Adults\n",
      "  True Positive Rate (TPR): 0.57\n",
      "  False Positive Rate (FPR): 0.07\n",
      "Age Group: Young Adults\n",
      "  True Positive Rate (TPR): 0.59\n",
      "  False Positive Rate (FPR): 0.07\n"
     ]
    }
   ],
   "source": [
    "for age in [0, 1]:\n",
    "    tpr, fpr = calculate_tpr_fpr_age(separacion_df, age)\n",
    "    age_label = 'Older Adults' if age == 0 else 'Young Adults'\n",
    "    print(f\"Age Group: {age_label}\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.2f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks5s1lMdhlP_"
   },
   "source": [
    "### Equalized Odds for **sex** atribute\n",
    "Map: [**0: Female**, **1: Male**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53CtaDwegUWH",
    "outputId": "8d4ee413-4204-4018-e461-9dcfec65a3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex Group: Female\n",
      "  True Positive Rate (TPR): 0.45\n",
      "  False Positive Rate (FPR): 0.02\n",
      "Sex Group: Male\n",
      "  True Positive Rate (TPR): 0.61\n",
      "  False Positive Rate (FPR): 0.10\n"
     ]
    }
   ],
   "source": [
    "for sex in [0, 1]:\n",
    "    tpr, fpr = calculate_tpr_fpr_sex(separacion_df, sex)\n",
    "    sex_label = 'Female' if sex == 0 else 'Male'\n",
    "    print(f\"Sex Group: {sex_label}\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.2f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtnPwLPQnAfm"
   },
   "source": [
    "## Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQAWZgLBKl1-",
    "outputId": "a3bccf47-cae9-4d93-b419-0a617501118d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged PPV age: 0.7272727272727273\n",
      "Unprivileged PPV age: 0.7467248908296943\n",
      "Predictive Parity Difference age: -0.019452163556966995\n",
      "Accuracy: 0.8323650871421572\n",
      "Privileged PPV: 0.7310683585755219\n",
      "Unprivileged PPV: 0.7130177514792899\n",
      "Predictive Parity Difference: 0.018050607096231963\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "classification_metric_age = ClassificationMetric(\n",
    "    df_test_aif,\n",
    "    df_test_pred_aif,\n",
    "    privileged_groups=[{\"age\": 1}],\n",
    "    unprivileged_groups=[{\"age\": 0}]\n",
    ")\n",
    "\n",
    "# Calcular el Valor Predictivo Positivo (PPV)\n",
    "privileged_ppv_age = classification_metric_age.positive_predictive_value(privileged=True)\n",
    "unprivileged_ppv_age = classification_metric_age.positive_predictive_value(privileged=False)\n",
    "\n",
    "predictive_parity_difference_age = privileged_ppv_age - unprivileged_ppv_age\n",
    "print(f\"Privileged PPV age: {privileged_ppv_age}\")\n",
    "print(f\"Unprivileged PPV age: {unprivileged_ppv_age}\")\n",
    "print(f\"Predictive Parity Difference age: {predictive_parity_difference_age}\")\n",
    "\n",
    "classification_metric_sex = ClassificationMetric(\n",
    "    df_test_aif,\n",
    "    df_test_pred_aif,\n",
    "    privileged_groups=[{\"sex\": 1}],\n",
    "    unprivileged_groups=[{\"sex\": 0}]\n",
    ")\n",
    "# Calcular el Valor Predictivo Positivo (PPV)\n",
    "privileged_ppv_sex = classification_metric_sex.positive_predictive_value(privileged=True)\n",
    "unprivileged_ppv_sex = classification_metric_sex.positive_predictive_value(privileged=False)\n",
    "\n",
    "\n",
    "predictive_parity_difference_sex = privileged_ppv_sex - unprivileged_ppv_sex\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy = accuracy_score(y_test, randomF_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Privileged PPV: {privileged_ppv_sex}\")\n",
    "print(f\"Unprivileged PPV: {unprivileged_ppv_sex}\")\n",
    "print(f\"Predictive Parity Difference: {predictive_parity_difference_sex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WENKgyhh44_"
   },
   "source": [
    "# Mitigación de Sesgos\n",
    "### Pre-procesamiento: **Reweighing**\n",
    "### In-procesamiento:\n",
    "### Post-procesamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFcG9Ttaicoi"
   },
   "source": [
    "## Pre-procesamiento: **Reweighing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevos pesos tras Reweighing:\n",
      "[1.09493593 0.8507266  0.8507266  0.78650541 1.09493593 1.09493593\n",
      " 1.09493593 0.8507266  1.09493593 0.8507266 ]\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from typing import List\n",
    "\n",
    "def reweighingPreprocessing(\n",
    "    train_aif_df: BinaryLabelDataset,\n",
    "    sensitive_features: List[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Aplica Reweighing a un conjunto de entrenamiento considerando múltiples atributos sensibles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear procesadores Reweighing para cada atributo sensible\n",
    "    reweigh_processors = []\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        reweigh_processors.append(\n",
    "            Reweighing(\n",
    "                unprivileged_groups=[{sensitive_feature: 0}],\n",
    "                privileged_groups=[{sensitive_feature: 1}]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Aplicar Reweighing a los pesos del dataset\n",
    "    actual_train_df = train_aif_df.copy()\n",
    "    \n",
    "    for reweigh_processor in reweigh_processors:\n",
    "        reweigh_processor: Reweighing\n",
    "        reweigh_processor.fit(actual_train_df)\n",
    "        actual_train_df = reweigh_processor.transform(actual_train_df)\n",
    "    \n",
    "    return actual_train_df\n",
    "\n",
    "# Aplicar Reweighing\n",
    "reweighted_train = reweighingPreprocessing(\n",
    "    df_train_aif,\n",
    "    ['age', 'sex']\n",
    ")\n",
    "\n",
    "# Mostrar los nuevos pesos después del preprocesamiento\n",
    "print(\"Nuevos pesos tras Reweighing:\")\n",
    "print(reweighted_train.instance_weights[:10])  # Muestra los primeros 10 ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression Model with Pre-processing: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logistic_model_pre = LogisticRegression(max_iter=5000, random_state=42)\n",
    "logistic_model_pre.fit(\n",
    "    reweighted_train.features, \n",
    "    reweighted_train.labels.ravel(), \n",
    "    sample_weight=reweighted_train.instance_weights\n",
    ")\n",
    "y_pred_logistic_pre = logistic_model_pre.predict(X_test_df)\n",
    "acc_lr_pre = accuracy_score(y_test, y_pred_logistic_pre)\n",
    "print(f'Accuracy for Logistic Regression Model with Pre-processing: {acc_lr_pre:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rS3vu7C2CEZ"
   },
   "source": [
    "## In-procesamiento: **Inserte Técnica Aquí**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import tensorflow as tf \n",
    "\n",
    "def adversarialDebiasingProcessing(\n",
    "    train_aif_df: BinaryLabelDataset,\n",
    "    test_aif_df: BinaryLabelDataset,\n",
    "    sensitive_features: List[str],\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 128,\n",
    "    adversary_loss_weight: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Función para entrenar un modelo debiasado usando AdversarialDebiasing.\n",
    "\n",
    "    Parámetros:\n",
    "    - train_aif_df (BinaryLabelDataset): Datos de entrenamiento.\n",
    "    - test_aif_df (BinaryLabelDataset): Datos de prueba.\n",
    "    - sensitive_features (List[str]): Lista de características sensibles (e.g., ['age', 'sex']).\n",
    "    - num_epochs (int): Número de épocas para entrenar.\n",
    "    - batch_size (int): Tamaño de lote durante el entrenamiento.\n",
    "    - adversary_loss_weight (float): Peso del adversario para reducir el sesgo.\n",
    "\n",
    "    Retorno:\n",
    "    - predicted_test_aif_df (BinaryLabelDataset): Predicciones debiasadas en el conjunto de prueba.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    sess = tf.compat.v1.Session()\n",
    "    debiased_models = []\n",
    "    predicted_test_aif_df = test_aif_df.copy()\n",
    "\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        # Configurar grupos privilegiados y no privilegiados\n",
    "        privileged_groups = [{sensitive_feature: 1}]\n",
    "        unprivileged_groups = [{sensitive_feature: 0}]\n",
    "        \n",
    "        # Crear y entrenar el modelo debiasado\n",
    "        debiased_model = AdversarialDebiasing(\n",
    "            privileged_groups=privileged_groups,\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            scope_name=f'debiased_classifier_{sensitive_feature}',\n",
    "            sess=sess,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            adversary_loss_weight=adversary_loss_weight\n",
    "        )\n",
    "        print(f\"Entrenando modelo debiasado para {sensitive_feature}...\")\n",
    "        debiased_model.fit(train_aif_df)\n",
    "        debiased_models.append(debiased_model)\n",
    "\n",
    "        # Generar predicciones\n",
    "        predicted_test_aif_df = debiased_model.predict(predicted_test_aif_df)\n",
    "\n",
    "    sess.close()\n",
    "    return predicted_test_aif_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo debiasado para age...\n",
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.810125; batch adversarial loss: 0.359171\n",
      "epoch 0; iter: 200; batch classifier loss: 0.364631; batch adversarial loss: 0.578823\n",
      "epoch 1; iter: 0; batch classifier loss: 0.373468; batch adversarial loss: 0.596967\n",
      "epoch 1; iter: 200; batch classifier loss: 0.397832; batch adversarial loss: 0.529721\n",
      "epoch 2; iter: 0; batch classifier loss: 0.374287; batch adversarial loss: 0.490988\n",
      "epoch 2; iter: 200; batch classifier loss: 0.393159; batch adversarial loss: 0.461242\n",
      "epoch 3; iter: 0; batch classifier loss: 0.411463; batch adversarial loss: 0.455661\n",
      "epoch 3; iter: 200; batch classifier loss: 0.468962; batch adversarial loss: 0.445357\n",
      "epoch 4; iter: 0; batch classifier loss: 0.471034; batch adversarial loss: 0.452916\n",
      "epoch 4; iter: 200; batch classifier loss: 0.655306; batch adversarial loss: 0.476814\n",
      "epoch 5; iter: 0; batch classifier loss: 0.549648; batch adversarial loss: 0.439400\n",
      "epoch 5; iter: 200; batch classifier loss: 0.435339; batch adversarial loss: 0.354212\n",
      "epoch 6; iter: 0; batch classifier loss: 0.345401; batch adversarial loss: 0.319270\n",
      "epoch 6; iter: 200; batch classifier loss: 0.366052; batch adversarial loss: 0.357718\n",
      "epoch 7; iter: 0; batch classifier loss: 0.261666; batch adversarial loss: 0.327834\n",
      "epoch 7; iter: 200; batch classifier loss: 0.332076; batch adversarial loss: 0.384723\n",
      "epoch 8; iter: 0; batch classifier loss: 0.293066; batch adversarial loss: 0.308843\n",
      "epoch 8; iter: 200; batch classifier loss: 0.239852; batch adversarial loss: 0.291422\n",
      "epoch 9; iter: 0; batch classifier loss: 0.336524; batch adversarial loss: 0.288091\n",
      "epoch 9; iter: 200; batch classifier loss: 0.286228; batch adversarial loss: 0.229476\n",
      "epoch 10; iter: 0; batch classifier loss: 0.308747; batch adversarial loss: 0.322573\n",
      "epoch 10; iter: 200; batch classifier loss: 0.324713; batch adversarial loss: 0.223112\n",
      "epoch 11; iter: 0; batch classifier loss: 0.411980; batch adversarial loss: 0.265440\n",
      "epoch 11; iter: 200; batch classifier loss: 0.333502; batch adversarial loss: 0.241222\n",
      "epoch 12; iter: 0; batch classifier loss: 0.272453; batch adversarial loss: 0.220507\n",
      "epoch 12; iter: 200; batch classifier loss: 0.309345; batch adversarial loss: 0.303611\n",
      "epoch 13; iter: 0; batch classifier loss: 0.349939; batch adversarial loss: 0.298289\n",
      "epoch 13; iter: 200; batch classifier loss: 0.331800; batch adversarial loss: 0.260366\n",
      "epoch 14; iter: 0; batch classifier loss: 0.291815; batch adversarial loss: 0.321586\n",
      "epoch 14; iter: 200; batch classifier loss: 0.257156; batch adversarial loss: 0.363405\n",
      "epoch 15; iter: 0; batch classifier loss: 0.276107; batch adversarial loss: 0.280699\n",
      "epoch 15; iter: 200; batch classifier loss: 0.357902; batch adversarial loss: 0.258582\n",
      "epoch 16; iter: 0; batch classifier loss: 0.325028; batch adversarial loss: 0.257783\n",
      "epoch 16; iter: 200; batch classifier loss: 0.254701; batch adversarial loss: 0.219596\n",
      "epoch 17; iter: 0; batch classifier loss: 0.293081; batch adversarial loss: 0.238231\n",
      "epoch 17; iter: 200; batch classifier loss: 0.307281; batch adversarial loss: 0.275510\n",
      "epoch 18; iter: 0; batch classifier loss: 0.221680; batch adversarial loss: 0.258846\n",
      "epoch 18; iter: 200; batch classifier loss: 0.261967; batch adversarial loss: 0.329120\n",
      "epoch 19; iter: 0; batch classifier loss: 0.313448; batch adversarial loss: 0.236081\n",
      "epoch 19; iter: 200; batch classifier loss: 0.311265; batch adversarial loss: 0.236835\n",
      "epoch 20; iter: 0; batch classifier loss: 0.353739; batch adversarial loss: 0.310484\n",
      "epoch 20; iter: 200; batch classifier loss: 0.355480; batch adversarial loss: 0.289905\n",
      "epoch 21; iter: 0; batch classifier loss: 0.298487; batch adversarial loss: 0.310677\n",
      "epoch 21; iter: 200; batch classifier loss: 0.301631; batch adversarial loss: 0.181067\n",
      "epoch 22; iter: 0; batch classifier loss: 0.276496; batch adversarial loss: 0.310284\n",
      "epoch 22; iter: 200; batch classifier loss: 0.354445; batch adversarial loss: 0.272339\n",
      "epoch 23; iter: 0; batch classifier loss: 0.284397; batch adversarial loss: 0.218533\n",
      "epoch 23; iter: 200; batch classifier loss: 0.367747; batch adversarial loss: 0.235323\n",
      "epoch 24; iter: 0; batch classifier loss: 0.246141; batch adversarial loss: 0.256828\n",
      "epoch 24; iter: 200; batch classifier loss: 0.334083; batch adversarial loss: 0.294793\n",
      "epoch 25; iter: 0; batch classifier loss: 0.311973; batch adversarial loss: 0.388983\n",
      "epoch 25; iter: 200; batch classifier loss: 0.294875; batch adversarial loss: 0.180433\n",
      "epoch 26; iter: 0; batch classifier loss: 0.306048; batch adversarial loss: 0.160005\n",
      "epoch 26; iter: 200; batch classifier loss: 0.244454; batch adversarial loss: 0.446952\n",
      "epoch 27; iter: 0; batch classifier loss: 0.279433; batch adversarial loss: 0.159955\n",
      "epoch 27; iter: 200; batch classifier loss: 0.405682; batch adversarial loss: 0.349713\n",
      "epoch 28; iter: 0; batch classifier loss: 0.263387; batch adversarial loss: 0.197025\n",
      "epoch 28; iter: 200; batch classifier loss: 0.298967; batch adversarial loss: 0.349785\n",
      "epoch 29; iter: 0; batch classifier loss: 0.327256; batch adversarial loss: 0.254315\n",
      "epoch 29; iter: 200; batch classifier loss: 0.297076; batch adversarial loss: 0.198233\n",
      "epoch 30; iter: 0; batch classifier loss: 0.282365; batch adversarial loss: 0.350340\n",
      "epoch 30; iter: 200; batch classifier loss: 0.282004; batch adversarial loss: 0.333299\n",
      "epoch 31; iter: 0; batch classifier loss: 0.325064; batch adversarial loss: 0.311319\n",
      "epoch 31; iter: 200; batch classifier loss: 0.281654; batch adversarial loss: 0.311759\n",
      "epoch 32; iter: 0; batch classifier loss: 0.295333; batch adversarial loss: 0.405444\n",
      "epoch 32; iter: 200; batch classifier loss: 0.260383; batch adversarial loss: 0.254055\n",
      "epoch 33; iter: 0; batch classifier loss: 0.353224; batch adversarial loss: 0.198243\n",
      "epoch 33; iter: 200; batch classifier loss: 0.235210; batch adversarial loss: 0.255546\n",
      "epoch 34; iter: 0; batch classifier loss: 0.322661; batch adversarial loss: 0.254983\n",
      "epoch 34; iter: 200; batch classifier loss: 0.341291; batch adversarial loss: 0.388166\n",
      "epoch 35; iter: 0; batch classifier loss: 0.313501; batch adversarial loss: 0.273854\n",
      "epoch 35; iter: 200; batch classifier loss: 0.344497; batch adversarial loss: 0.312835\n",
      "epoch 36; iter: 0; batch classifier loss: 0.269459; batch adversarial loss: 0.274192\n",
      "epoch 36; iter: 200; batch classifier loss: 0.294638; batch adversarial loss: 0.292962\n",
      "epoch 37; iter: 0; batch classifier loss: 0.287606; batch adversarial loss: 0.273962\n",
      "epoch 37; iter: 200; batch classifier loss: 0.273059; batch adversarial loss: 0.274318\n",
      "epoch 38; iter: 0; batch classifier loss: 0.290264; batch adversarial loss: 0.256493\n",
      "epoch 38; iter: 200; batch classifier loss: 0.397548; batch adversarial loss: 0.332005\n",
      "epoch 39; iter: 0; batch classifier loss: 0.278208; batch adversarial loss: 0.273958\n",
      "epoch 39; iter: 200; batch classifier loss: 0.315426; batch adversarial loss: 0.292481\n",
      "epoch 40; iter: 0; batch classifier loss: 0.244092; batch adversarial loss: 0.254633\n",
      "epoch 40; iter: 200; batch classifier loss: 0.347343; batch adversarial loss: 0.293266\n",
      "epoch 41; iter: 0; batch classifier loss: 0.297682; batch adversarial loss: 0.311653\n",
      "epoch 41; iter: 200; batch classifier loss: 0.334800; batch adversarial loss: 0.274089\n",
      "epoch 42; iter: 0; batch classifier loss: 0.270711; batch adversarial loss: 0.273755\n",
      "epoch 42; iter: 200; batch classifier loss: 0.321712; batch adversarial loss: 0.236319\n",
      "epoch 43; iter: 0; batch classifier loss: 0.259316; batch adversarial loss: 0.216885\n",
      "epoch 43; iter: 200; batch classifier loss: 0.226642; batch adversarial loss: 0.197978\n",
      "epoch 44; iter: 0; batch classifier loss: 0.351206; batch adversarial loss: 0.311902\n",
      "epoch 44; iter: 200; batch classifier loss: 0.262925; batch adversarial loss: 0.236938\n",
      "epoch 45; iter: 0; batch classifier loss: 0.311250; batch adversarial loss: 0.236366\n",
      "epoch 45; iter: 200; batch classifier loss: 0.268578; batch adversarial loss: 0.254715\n",
      "epoch 46; iter: 0; batch classifier loss: 0.204503; batch adversarial loss: 0.254497\n",
      "epoch 46; iter: 200; batch classifier loss: 0.297944; batch adversarial loss: 0.331003\n",
      "epoch 47; iter: 0; batch classifier loss: 0.258701; batch adversarial loss: 0.407295\n",
      "epoch 47; iter: 200; batch classifier loss: 0.268236; batch adversarial loss: 0.274386\n",
      "epoch 48; iter: 0; batch classifier loss: 0.253688; batch adversarial loss: 0.274081\n",
      "epoch 48; iter: 200; batch classifier loss: 0.235345; batch adversarial loss: 0.255132\n",
      "epoch 49; iter: 0; batch classifier loss: 0.376634; batch adversarial loss: 0.216483\n",
      "epoch 49; iter: 200; batch classifier loss: 0.312640; batch adversarial loss: 0.274206\n",
      "Entrenando modelo debiasado para sex...\n",
      "epoch 0; iter: 0; batch classifier loss: 0.663146; batch adversarial loss: 0.649218\n",
      "epoch 0; iter: 200; batch classifier loss: 0.572147; batch adversarial loss: 0.689655\n",
      "epoch 1; iter: 0; batch classifier loss: 0.373249; batch adversarial loss: 0.655742\n",
      "epoch 1; iter: 200; batch classifier loss: 0.421429; batch adversarial loss: 0.659030\n",
      "epoch 2; iter: 0; batch classifier loss: 0.378796; batch adversarial loss: 0.672240\n",
      "epoch 2; iter: 200; batch classifier loss: 0.400665; batch adversarial loss: 0.621438\n",
      "epoch 3; iter: 0; batch classifier loss: 0.369202; batch adversarial loss: 0.637856\n",
      "epoch 3; iter: 200; batch classifier loss: 0.371793; batch adversarial loss: 0.635061\n",
      "epoch 4; iter: 0; batch classifier loss: 0.344417; batch adversarial loss: 0.664502\n",
      "epoch 4; iter: 200; batch classifier loss: 0.362836; batch adversarial loss: 0.638035\n",
      "epoch 5; iter: 0; batch classifier loss: 0.405162; batch adversarial loss: 0.620262\n",
      "epoch 5; iter: 200; batch classifier loss: 0.355642; batch adversarial loss: 0.624676\n",
      "epoch 6; iter: 0; batch classifier loss: 0.265500; batch adversarial loss: 0.599296\n",
      "epoch 6; iter: 200; batch classifier loss: 0.380398; batch adversarial loss: 0.637088\n",
      "epoch 7; iter: 0; batch classifier loss: 0.314070; batch adversarial loss: 0.647538\n",
      "epoch 7; iter: 200; batch classifier loss: 0.291594; batch adversarial loss: 0.650476\n",
      "epoch 8; iter: 0; batch classifier loss: 0.347814; batch adversarial loss: 0.630716\n",
      "epoch 8; iter: 200; batch classifier loss: 0.374924; batch adversarial loss: 0.619707\n",
      "epoch 9; iter: 0; batch classifier loss: 0.308708; batch adversarial loss: 0.577439\n",
      "epoch 9; iter: 200; batch classifier loss: 0.282937; batch adversarial loss: 0.626022\n",
      "epoch 10; iter: 0; batch classifier loss: 0.433300; batch adversarial loss: 0.617156\n",
      "epoch 10; iter: 200; batch classifier loss: 0.306378; batch adversarial loss: 0.674939\n",
      "epoch 11; iter: 0; batch classifier loss: 0.387497; batch adversarial loss: 0.651270\n",
      "epoch 11; iter: 200; batch classifier loss: 0.298829; batch adversarial loss: 0.632702\n",
      "epoch 12; iter: 0; batch classifier loss: 0.328681; batch adversarial loss: 0.602770\n",
      "epoch 12; iter: 200; batch classifier loss: 0.300304; batch adversarial loss: 0.633651\n",
      "epoch 13; iter: 0; batch classifier loss: 0.356306; batch adversarial loss: 0.649189\n",
      "epoch 13; iter: 200; batch classifier loss: 0.261081; batch adversarial loss: 0.613711\n",
      "epoch 14; iter: 0; batch classifier loss: 0.340175; batch adversarial loss: 0.619969\n",
      "epoch 14; iter: 200; batch classifier loss: 0.288170; batch adversarial loss: 0.614369\n",
      "epoch 15; iter: 0; batch classifier loss: 0.266635; batch adversarial loss: 0.634043\n",
      "epoch 15; iter: 200; batch classifier loss: 0.405163; batch adversarial loss: 0.597805\n",
      "epoch 16; iter: 0; batch classifier loss: 0.355455; batch adversarial loss: 0.574930\n",
      "epoch 16; iter: 200; batch classifier loss: 0.353754; batch adversarial loss: 0.685515\n",
      "epoch 17; iter: 0; batch classifier loss: 0.251115; batch adversarial loss: 0.677939\n",
      "epoch 17; iter: 200; batch classifier loss: 0.266104; batch adversarial loss: 0.619391\n",
      "epoch 18; iter: 0; batch classifier loss: 0.275414; batch adversarial loss: 0.589266\n",
      "epoch 18; iter: 200; batch classifier loss: 0.319323; batch adversarial loss: 0.620219\n",
      "epoch 19; iter: 0; batch classifier loss: 0.361718; batch adversarial loss: 0.632297\n",
      "epoch 19; iter: 200; batch classifier loss: 0.282628; batch adversarial loss: 0.585507\n",
      "epoch 20; iter: 0; batch classifier loss: 0.340383; batch adversarial loss: 0.689301\n",
      "epoch 20; iter: 200; batch classifier loss: 0.278510; batch adversarial loss: 0.603856\n",
      "epoch 21; iter: 0; batch classifier loss: 0.403769; batch adversarial loss: 0.603504\n",
      "epoch 21; iter: 200; batch classifier loss: 0.394909; batch adversarial loss: 0.639034\n",
      "epoch 22; iter: 0; batch classifier loss: 0.339168; batch adversarial loss: 0.591753\n",
      "epoch 22; iter: 200; batch classifier loss: 0.259189; batch adversarial loss: 0.595411\n",
      "epoch 23; iter: 0; batch classifier loss: 0.338960; batch adversarial loss: 0.664491\n",
      "epoch 23; iter: 200; batch classifier loss: 0.256981; batch adversarial loss: 0.674027\n",
      "epoch 24; iter: 0; batch classifier loss: 0.263839; batch adversarial loss: 0.639601\n",
      "epoch 24; iter: 200; batch classifier loss: 0.329853; batch adversarial loss: 0.642768\n",
      "epoch 25; iter: 0; batch classifier loss: 0.280851; batch adversarial loss: 0.659044\n",
      "epoch 25; iter: 200; batch classifier loss: 0.373589; batch adversarial loss: 0.625011\n",
      "epoch 26; iter: 0; batch classifier loss: 0.328475; batch adversarial loss: 0.589244\n",
      "epoch 26; iter: 200; batch classifier loss: 0.326206; batch adversarial loss: 0.696418\n",
      "epoch 27; iter: 0; batch classifier loss: 0.280176; batch adversarial loss: 0.635487\n",
      "epoch 27; iter: 200; batch classifier loss: 0.286887; batch adversarial loss: 0.615770\n",
      "epoch 28; iter: 0; batch classifier loss: 0.302231; batch adversarial loss: 0.582751\n",
      "epoch 28; iter: 200; batch classifier loss: 0.269267; batch adversarial loss: 0.607894\n",
      "epoch 29; iter: 0; batch classifier loss: 0.273532; batch adversarial loss: 0.600300\n",
      "epoch 29; iter: 200; batch classifier loss: 0.312127; batch adversarial loss: 0.623526\n",
      "epoch 30; iter: 0; batch classifier loss: 0.330819; batch adversarial loss: 0.673289\n",
      "epoch 30; iter: 200; batch classifier loss: 0.421678; batch adversarial loss: 0.643101\n",
      "epoch 31; iter: 0; batch classifier loss: 0.274694; batch adversarial loss: 0.597517\n",
      "epoch 31; iter: 200; batch classifier loss: 0.302731; batch adversarial loss: 0.592358\n",
      "epoch 32; iter: 0; batch classifier loss: 0.313674; batch adversarial loss: 0.604380\n",
      "epoch 32; iter: 200; batch classifier loss: 0.311110; batch adversarial loss: 0.631251\n",
      "epoch 33; iter: 0; batch classifier loss: 0.298024; batch adversarial loss: 0.647216\n",
      "epoch 33; iter: 200; batch classifier loss: 0.274220; batch adversarial loss: 0.608896\n",
      "epoch 34; iter: 0; batch classifier loss: 0.281754; batch adversarial loss: 0.639672\n",
      "epoch 34; iter: 200; batch classifier loss: 0.354390; batch adversarial loss: 0.582453\n",
      "epoch 35; iter: 0; batch classifier loss: 0.220123; batch adversarial loss: 0.665672\n",
      "epoch 35; iter: 200; batch classifier loss: 0.413450; batch adversarial loss: 0.664662\n",
      "epoch 36; iter: 0; batch classifier loss: 0.353305; batch adversarial loss: 0.575222\n",
      "epoch 36; iter: 200; batch classifier loss: 0.255307; batch adversarial loss: 0.588517\n",
      "epoch 37; iter: 0; batch classifier loss: 0.277511; batch adversarial loss: 0.658273\n",
      "epoch 37; iter: 200; batch classifier loss: 0.386585; batch adversarial loss: 0.618167\n",
      "epoch 38; iter: 0; batch classifier loss: 0.326526; batch adversarial loss: 0.633507\n",
      "epoch 38; iter: 200; batch classifier loss: 0.226302; batch adversarial loss: 0.681283\n",
      "epoch 39; iter: 0; batch classifier loss: 0.343064; batch adversarial loss: 0.646907\n",
      "epoch 39; iter: 200; batch classifier loss: 0.392465; batch adversarial loss: 0.566460\n",
      "epoch 40; iter: 0; batch classifier loss: 0.341844; batch adversarial loss: 0.679255\n",
      "epoch 40; iter: 200; batch classifier loss: 0.285090; batch adversarial loss: 0.595676\n",
      "epoch 41; iter: 0; batch classifier loss: 0.254768; batch adversarial loss: 0.613006\n",
      "epoch 41; iter: 200; batch classifier loss: 0.361947; batch adversarial loss: 0.622797\n",
      "epoch 42; iter: 0; batch classifier loss: 0.349103; batch adversarial loss: 0.639320\n",
      "epoch 42; iter: 200; batch classifier loss: 0.387437; batch adversarial loss: 0.675931\n",
      "epoch 43; iter: 0; batch classifier loss: 0.353540; batch adversarial loss: 0.634313\n",
      "epoch 43; iter: 200; batch classifier loss: 0.327489; batch adversarial loss: 0.602015\n",
      "epoch 44; iter: 0; batch classifier loss: 0.277028; batch adversarial loss: 0.605843\n",
      "epoch 44; iter: 200; batch classifier loss: 0.269276; batch adversarial loss: 0.637836\n",
      "epoch 45; iter: 0; batch classifier loss: 0.241796; batch adversarial loss: 0.608516\n",
      "epoch 45; iter: 200; batch classifier loss: 0.269640; batch adversarial loss: 0.622783\n",
      "epoch 46; iter: 0; batch classifier loss: 0.347814; batch adversarial loss: 0.625526\n",
      "epoch 46; iter: 200; batch classifier loss: 0.396270; batch adversarial loss: 0.607542\n",
      "epoch 47; iter: 0; batch classifier loss: 0.385019; batch adversarial loss: 0.694552\n",
      "epoch 47; iter: 200; batch classifier loss: 0.306377; batch adversarial loss: 0.617116\n",
      "epoch 48; iter: 0; batch classifier loss: 0.421002; batch adversarial loss: 0.647533\n",
      "epoch 48; iter: 200; batch classifier loss: 0.413675; batch adversarial loss: 0.583891\n",
      "epoch 49; iter: 0; batch classifier loss: 0.297552; batch adversarial loss: 0.614082\n",
      "epoch 49; iter: 200; batch classifier loss: 0.381643; batch adversarial loss: 0.668569\n"
     ]
    }
   ],
   "source": [
    "predicted_test_inprocessing = adversarialDebiasingProcessing(\n",
    "    train_aif_df=df_train_aif,  # Datos de entrenamiento\n",
    "    test_aif_df=df_test_aif,    # Datos de prueba\n",
    "    sensitive_features=['age', 'sex'],  # Atributos sensibles\n",
    "    num_epochs=50,  # Configuración del número de épocas\n",
    "    batch_size=128,  # Tamaño de lote\n",
    "    adversary_loss_weight=0.1  # Peso del adversario\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9UG6EWE2Pv8"
   },
   "source": [
    "## Post-procesamiento: **Equalized Odds Post-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "fPvO1_bP2T7_"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "#Código Post-Procesamiento\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "# Cramos una función que se encargue de reajustar las predicciones\n",
    "def eqOddsPredictionProccesing(\n",
    "    test_aif_df: BinaryLabelDataset,\n",
    "    test_pred_aif_df: BinaryLabelDataset,\n",
    "    sensitive_features: List[str],\n",
    "):\n",
    "\n",
    "    eq_odds_processers = []\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        eq_odds_processers.append(\n",
    "            EqOddsPostprocessing(\n",
    "                unprivileged_groups=[{sensitive_feature: 0}],\n",
    "                privileged_groups=[{sensitive_feature: 1}],\n",
    "                seed=42\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    actual_pred_aif_df = test_pred_aif_df.copy()\n",
    "    \n",
    "    for eq_odds_processer in eq_odds_processers:\n",
    "        eq_odds_processer: EqOddsPostprocessing\n",
    "        eq_odds_processer.fit(test_aif_df, actual_pred_aif_df)\n",
    "        actual_pred_aif_df = eq_odds_processer.predict(actual_pred_aif_df)\n",
    "        \n",
    "    return actual_pred_aif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact for age: 1.0650931315940388\n",
      "Dispate Impact for sex: 0.6202328376320687\n",
      "New Accuracy: 0.8115069643732065\n"
     ]
    }
   ],
   "source": [
    "post_processed_preds = eqOddsPredictionProccesing(\n",
    "    df_test_aif,\n",
    "    df_test_pred_aif, \n",
    "    ['age', 'sex']\n",
    ")\n",
    "\n",
    "# Para 'age'\n",
    "metric_age = ClassificationMetric(\n",
    "    df_test_aif,\n",
    "    post_processed_preds,\n",
    "    unprivileged_groups=[{\"age\": 0}],\n",
    "    privileged_groups=[{\"age\": 1}]\n",
    ")\n",
    "\n",
    "# Para 'sex'\n",
    "metric_sex = ClassificationMetric(\n",
    "    df_test_aif,\n",
    "    post_processed_preds,\n",
    "    unprivileged_groups=[{\"sex\": 0}],\n",
    "    privileged_groups=[{\"sex\": 1}]\n",
    ")\n",
    "\n",
    "print(\"Disparate Impact for age:\", metric_age.disparate_impact())\n",
    "print(\"Dispate Impact for sex:\", metric_sex.disparate_impact())\n",
    "new_accuracy = accuracy_score(y_test, post_processed_preds.labels)\n",
    "print(f\"New Accuracy: {new_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHcG02a8lt4W"
   },
   "source": [
    "# Medición de Mitigación de Sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRQWwinZ2Yij"
   },
   "source": [
    "## Combinación 1: Pre-procesamiento + In-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "czYk1bQ80dx9"
   },
   "outputs": [],
   "source": [
    "## Código que combine las dos técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tywIOpMw0l4k"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7jAqwb8l6XB"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "WEvI3mlylObY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_test_fitted_pre \u001b[38;5;241m=\u001b[39m \u001b[43mX_test_pre\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m X_test_fitted_pre \u001b[38;5;241m=\u001b[39m X_test_fitted_pre[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m X_test_fitted_pre\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred_logistic_pre\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test_pre' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_fitted_pre = X_test_pre.copy()\n",
    "\n",
    "X_test_fitted_pre = X_test_fitted_pre[['age', 'sex']]\n",
    "X_test_fitted_pre.loc[:, 'y_pred'] = y_pred_logistic_pre\n",
    "X_test_fitted_pre['y_true'] = y_test_pre.values\n",
    "X_test_fitted_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l92PisY7ltZe"
   },
   "outputs": [],
   "source": [
    "for age in [0, 1]:\n",
    "    tpr, fpr = calculate_tpr_fpr_age(X_test_fitted_pre, age)\n",
    "    age_label = 'Older Adults' if age == 0 else 'Young Adults'\n",
    "    print(f\"Age Group: {age_label}\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.2f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-_iOE3wmIoa"
   },
   "outputs": [],
   "source": [
    "for sex in [0, 1]:\n",
    "    tpr, fpr = calculate_tpr_fpr_sex(X_test_fitted_pre, sex)\n",
    "    sex_label = 'Female' if sex == 0 else 'Male'\n",
    "    print(f\"Sex Group: {sex_label}\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.2f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIWoDqa30mHI"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSQYOgL504SD"
   },
   "source": [
    "## Combinación 2: In-procesamiento + Post-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyd97ICz083J"
   },
   "outputs": [],
   "source": [
    "## Código que combine las dos técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr2hhtAJ1HME"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NLywt1Z1Oju"
   },
   "outputs": [],
   "source": [
    "# Código de Independencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyfS8ENZ1LEU"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2aTbcPN1Qsq"
   },
   "outputs": [],
   "source": [
    "# Código de Separación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du0nPRCR1CnU"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cje5Xbu91CC5"
   },
   "outputs": [],
   "source": [
    "# Código de Suficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDyfGMxM1ayd"
   },
   "source": [
    "## Combinación 3: Pre-procesamiento + Post-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9O9-ukl1aye"
   },
   "outputs": [],
   "source": [
    "## Código que combine las dos técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gld5EHJy1aye"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "il05ry-y1ayf"
   },
   "outputs": [],
   "source": [
    "# Código de Independencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2O_Oc511ayf"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8gtxWls1ayf"
   },
   "outputs": [],
   "source": [
    "# Código de Separación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USapjYuh1ayf"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6e5O5-a1ayf"
   },
   "outputs": [],
   "source": [
    "# Código de Suficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmNHfI0d1tyZ"
   },
   "source": [
    "## Combinación 4: Pre-procesamiento + In-Procesamiento + Post-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L3bpAGk1tyZ"
   },
   "outputs": [],
   "source": [
    "## Código que combine las dos técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1x4Qg6m1tya"
   },
   "source": [
    "### Independencia (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UduOtFUO1tya"
   },
   "outputs": [],
   "source": [
    "# Código de Independencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sth3VWqq1tya"
   },
   "source": [
    "### Separación (Equalized Odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hPLa-Sh1tya"
   },
   "outputs": [],
   "source": [
    "# Código de Separación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue09jjXI1tya"
   },
   "source": [
    "### Suficiencia (Predictive Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yRr99hB1tyb"
   },
   "outputs": [],
   "source": [
    "# Código de Suficiencia"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
